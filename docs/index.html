<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>nsc 0.1.0 documentation</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="#" class="icon icon-home"> nsc
          </a>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-readme">Neural spell checking using Transformers and Graph Neural Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-models">Pretrained models</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-reproduce">Training and reproducing results</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-nsc">nsc package</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="#">nsc</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="#" class="icon icon-home"></a> &raquo;</li>
      <li>nsc 0.1.0 documentation</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <div class="toctree-wrapper compound">
<span id="document-readme"></span><section id="neural-spell-checking-using-transformers-and-graph-neural-networks">
<h1>Neural spell checking using Transformers and Graph Neural Networks<a class="headerlink" href="#neural-spell-checking-using-transformers-and-graph-neural-networks" title="Permalink to this headline"></a></h1>
<p>This project is about detecting and correcting spelling errors using Transformers and
Graph Neural Networks. Visit the <a class="reference external" href="https://bastiscode.github.io/spell_check">documentation</a> (which also includes this README)
for information on how to reproduce results and train your own models
as well as a more detailed description of the <code class="docutils literal notranslate"><span class="pre">nsc</span></code> <a class="reference internal" href="#python-api">Python API</a>.</p>
<section id="installation">
<h2>Installation<a class="headerlink" href="#installation" title="Permalink to this headline"></a></h2>
<p>Clone the repository</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>git clone git@github.com:bastiscode/spell_check.git
</pre></div>
</div>
<p>Install from source (alternatively you can use <a class="reference internal" href="#docker">Docker</a>)</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>make install
</pre></div>
</div>
</section>
<section id="usage">
<h2>Usage<a class="headerlink" href="#usage" title="Permalink to this headline"></a></h2>
<p>There are two main ways to use this project.
Either via the command line or by directly using the Python API.</p>
<section id="command-line-interfaces">
<h3>Command line interfaces<a class="headerlink" href="#command-line-interfaces" title="Permalink to this headline"></a></h3>
<p>After installation there will be three commands available in your environment:</p>
<ol class="arabic simple">
<li><p><code class="docutils literal notranslate"><span class="pre">nsec</span></code> for neural spelling error correction</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">nsed</span></code> for neural spelling error detection</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">ntr</span></code> for neural tokenization repair</p></li>
</ol>
<p>By default all three commands take input from <cite>stdin</cite>, run their respective task on the
input line by line and print their output line by line to <cite>stdout</cite>.</p>
<p><strong>Spelling error correction using</strong> <code class="docutils literal notranslate"><span class="pre">nsec</span></code></p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># correct text by piping into nsec,</span>
<span class="nb">echo</span> <span class="s2">&quot;This is an incorect sentense!&quot;</span> <span class="p">|</span> nsec
cat path/to/file.txt <span class="p">|</span> nsec

<span class="c1"># by using the -c flag</span>
nsec -c <span class="s2">&quot;This is an incorect sentense!&quot;</span>

<span class="c1"># or by passing a file</span>
nsec -f path/to/file.txt
</pre></div>
</div>
<p><strong>Spelling error detection using</strong> <code class="docutils literal notranslate"><span class="pre">nsed</span></code></p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># detect errors by piping into nsed,</span>
<span class="nb">echo</span> <span class="s2">&quot;This is an incorect sentense!&quot;</span> <span class="p">|</span> nsed
cat path/to/file.txt <span class="p">|</span> nsed

<span class="c1"># by using the -d flag</span>
nsed -d <span class="s2">&quot;This is an incorect sentense!&quot;</span>

<span class="c1"># or by passing a file</span>
nsed -f path/to/file.txt
</pre></div>
</div>
<p><strong>Tokenization repair using</strong> <code class="docutils literal notranslate"><span class="pre">ntr</span></code></p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># repair text by piping into ntr,</span>
<span class="nb">echo</span> <span class="s2">&quot;Thisis an inc orect sentens e!&quot;</span> <span class="p">|</span> ntr
cat path/to/file.txt <span class="p">|</span> ntr

<span class="c1"># by using the -r flag</span>
ntr -r <span class="s2">&quot;Thisis an inc orect sentens e!&quot;</span>

<span class="c1"># or by passing a file</span>
ntr -f path/to/file.txt
</pre></div>
</div>
<p>You can also combine the <code class="docutils literal notranslate"><span class="pre">ntr</span></code>, <code class="docutils literal notranslate"><span class="pre">nsed</span></code>, and <code class="docutils literal notranslate"><span class="pre">nsec</span></code> commands in a variety of ways.
Some examples are shown below.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># repair and detect</span>
<span class="nb">echo</span> <span class="s2">&quot;Repi arand detec erors in tihssen tence!&quot;</span> <span class="p">|</span> ntr <span class="p">|</span> nsed

<span class="c1"># repair and correct</span>
<span class="nb">echo</span> <span class="s2">&quot;Repi arand core ct tihssen tens!&quot;</span> <span class="p">|</span> ntr <span class="p">|</span> nsec

<span class="c1"># repair, detect and correct</span>
<span class="c1"># (this pipeline uses the spelling error detection output</span>
<span class="c1"># to guide the spelling error correction model to correct only the misspelled words)</span>
<span class="nb">echo</span> <span class="s2">&quot;Repi arand core ct tihssen tens!&quot;</span> <span class="p">|</span> ntr <span class="p">|</span> nsed --sec-out <span class="p">|</span> nsec --sed-in

<span class="c1"># repair and correct a file and save the output</span>
ntr -f path/to/file.txt <span class="p">|</span> nsec --progress -o path/to/output_file.txt
</pre></div>
</div>
<p>There are a few other command line options available for the <code class="docutils literal notranslate"><span class="pre">nsec</span></code>, <code class="docutils literal notranslate"><span class="pre">nsed</span></code> and <code class="docutils literal notranslate"><span class="pre">ntr</span></code> commands. Inspect
them by passing the <code class="docutils literal notranslate"><span class="pre">-h</span> <span class="pre">/</span> <span class="pre">--help</span></code> flag to the commands.</p>
</section>
<section id="python-api">
<h3>Python API<a class="headerlink" href="#python-api" title="Permalink to this headline"></a></h3>
<p>We also provide a Python API for you to use spell checking models directly in code. Below are basic
code examples on how to use the API.</p>
<p><strong>Spelling error correction</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">nsc</span> <span class="kn">import</span> <span class="n">SpellingErrorCorrector</span><span class="p">,</span> <span class="n">get_available_spelling_error_correction_models</span>

<span class="c1"># show all spelling error correction models</span>
<span class="nb">print</span><span class="p">(</span><span class="n">get_available_spelling_error_correction_models</span><span class="p">())</span>

<span class="c1"># use a pretrained model</span>
<span class="n">sec</span> <span class="o">=</span> <span class="n">SpellingErrorCorrector</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">()</span>
<span class="c1"># correct errors in text</span>
<span class="n">correction</span> <span class="o">=</span> <span class="n">sec</span><span class="o">.</span><span class="n">correct_text</span><span class="p">(</span><span class="s2">&quot;Tihs text has erors!&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">correction</span><span class="p">)</span>
<span class="c1"># correct errors in file</span>
<span class="n">corrections</span> <span class="o">=</span> <span class="n">sec</span><span class="o">.</span><span class="n">correct_file</span><span class="p">(</span><span class="s2">&quot;path/to/file.txt&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">correction</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Spelling error detection</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">nsc</span> <span class="kn">import</span> <span class="n">SpellingErrorDetector</span><span class="p">,</span> <span class="n">get_available_spelling_error_detection_models</span>

<span class="c1"># show all spelling error detection models</span>
<span class="nb">print</span><span class="p">(</span><span class="n">get_available_spelling_error_detection_models</span><span class="p">())</span>

<span class="c1"># use a pretrained model</span>
<span class="n">sed</span> <span class="o">=</span> <span class="n">SpellingErrorDetector</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">()</span>
<span class="c1"># detect errors in text</span>
<span class="n">detection</span> <span class="o">=</span> <span class="n">sed</span><span class="o">.</span><span class="n">detect_text</span><span class="p">(</span><span class="s2">&quot;Tihs text has erors!&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">detection</span><span class="p">)</span>
<span class="c1"># detect errors in file</span>
<span class="n">detections</span> <span class="o">=</span> <span class="n">sed</span><span class="o">.</span><span class="n">detect_file</span><span class="p">(</span><span class="s2">&quot;path/to/file.txt&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">detections</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Tokenization repair</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">nsc</span> <span class="kn">import</span> <span class="n">TokenizationRepairer</span><span class="p">,</span> <span class="n">get_available_tokenization_repair_models</span>

<span class="c1"># show all tokenization repair models</span>
<span class="nb">print</span><span class="p">(</span><span class="n">get_available_tokenization_repair_models</span><span class="p">())</span>

<span class="c1"># use a pretrained model</span>
<span class="n">tr</span> <span class="o">=</span> <span class="n">TokenizationRepairer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">()</span>
<span class="c1"># repair tokenization in text</span>
<span class="n">repaired_text</span> <span class="o">=</span> <span class="n">tr</span><span class="o">.</span><span class="n">repair_text</span><span class="p">(</span><span class="s2">&quot;Ti hstext h aserors!&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">repaired_text</span><span class="p">)</span>
<span class="c1"># repair tokenization in file</span>
<span class="n">repaired_file</span> <span class="o">=</span> <span class="n">tr</span><span class="o">.</span><span class="n">repair_file</span><span class="p">(</span><span class="s2">&quot;path/to/file.txt&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">repaired_file</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="docker">
<h2>Docker<a class="headerlink" href="#docker" title="Permalink to this headline"></a></h2>
<p>This project can also be run using Docker.
Inside the Docker container both the <a class="reference internal" href="#command-line-interfaces">Command line interfaces</a> and <a class="reference internal" href="#python-api">Python API</a> are available for you to use.
You can also evaluate model predictions on benchmarks.</p>
<p>To build the Docker image</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>make build_docker
</pre></div>
</div>
<p>To start a Docker container</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># run the docker container with GPU support</span>
make run_docker_gpu
<span class="c1"># or with CPU support</span>
make run_docker_cpu
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The Docker setup is only intended to be used for using the command line tools/Python API with pretrained or
your own models and evaluating them on benchmarks, but not for training.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Running the Docker container with GPU support assumes that you have the <a class="reference external" href="https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html">NVIDIA Container Toolkit</a> installed.</p>
</div>
</section>
</section>
<span id="document-models"></span><section id="pretrained-models">
<h1>Pretrained models<a class="headerlink" href="#pretrained-models" title="Permalink to this headline"></a></h1>
<p>The following table shows all pretrained models available for the CLI tools and Python API:</p>
<table class="docutils align-default" id="id1">
<caption><span class="caption-text">Table</span><a class="headerlink" href="#id1" title="Permalink to this table"></a></caption>
<colgroup>
<col style="width: 25%" />
<col style="width: 25%" />
<col style="width: 25%" />
<col style="width: 25%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Task (CLI tool/Python class)</p></th>
<th class="head"><p>Model</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Default</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>tokenization_repair (<code class="docutils literal notranslate"><span class="pre">ntr</span></code>/<code class="docutils literal notranslate"><span class="pre">TokenizationRepairer</span></code>)</p></td>
<td><p>transformer_eo</p></td>
<td><p>Transformer model that repairs sequences by predicting repair tokens for each character.</p></td>
<td><p>X</p></td>
</tr>
<tr class="row-odd"><td><p>tokenization_repair (<code class="docutils literal notranslate"><span class="pre">ntr</span></code>/<code class="docutils literal notranslate"><span class="pre">TokenizationRepairer</span></code>)</p></td>
<td><p>tokenization_repair+</p></td>
<td><p>Transformer model that repairs sequences by predicting repair tokens for each character. Different from transformer_eo because this model also was trained to detect spelling errors.</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>tokenization_repair (<code class="docutils literal notranslate"><span class="pre">ntr</span></code>/<code class="docutils literal notranslate"><span class="pre">TokenizationRepairer</span></code>)</p></td>
<td><p>tokenization_repair++</p></td>
<td><p>Transformer model that repairs sequences by predicting repair tokens for each character. Different from transformer_eo because this model also was trained to detect and correct spelling errors.</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>sed_sequence (<code class="docutils literal notranslate"><span class="pre">nsed</span></code>/<code class="docutils literal notranslate"><span class="pre">SpellingErrorDetector</span></code>)</p></td>
<td><p>transformer</p></td>
<td><p>Regular transformer processing a sequence of sub-word tokens. Predicts spelling errors on sequence level using the aggregated representations of all sub-word tokens.</p></td>
<td><p>X</p></td>
</tr>
<tr class="row-even"><td><p>sed_sequence (<code class="docutils literal notranslate"><span class="pre">nsed</span></code>/<code class="docutils literal notranslate"><span class="pre">SpellingErrorDetector</span></code>)</p></td>
<td><p>transformer+</p></td>
<td><p>Regular transformer processing a sequence of sub-word tokens. Before predicting spelling errors, sub-word representations within a word are aggregated and enriched with word features to obtain word representations. Predicts spelling errors on sequence level using the aggregation of those word representations.</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>sed_sequence (<code class="docutils literal notranslate"><span class="pre">nsed</span></code>/<code class="docutils literal notranslate"><span class="pre">SpellingErrorDetector</span></code>)</p></td>
<td><p>gnn</p></td>
<td><p>Attentional Graph Neural Network which processes language graphs with fully connected word nodes and fully connected sub-word cliques. Predicts spelling errors on sequence level using the aggregated representations of all word nodes.</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>sed_sequence (<code class="docutils literal notranslate"><span class="pre">nsed</span></code>/<code class="docutils literal notranslate"><span class="pre">SpellingErrorDetector</span></code>)</p></td>
<td><p>gnn+</p></td>
<td><p>Attentional Graph Neural Network which processes language graphs with fully connected word nodes, word features and fully connected sub-word cliques. Predicts spelling errors on sequence level using the aggregated representations of all word nodes.</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>sed_words (<code class="docutils literal notranslate"><span class="pre">nsed</span></code>/<code class="docutils literal notranslate"><span class="pre">SpellingErrorDetector</span></code>)</p></td>
<td><p>transformer</p></td>
<td><p>Regular transformer processing a sequence of sub-word tokens. Predicts spelling errors on word level using the aggregated sub-word representations per word.</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>sed_words (<code class="docutils literal notranslate"><span class="pre">nsed</span></code>/<code class="docutils literal notranslate"><span class="pre">SpellingErrorDetector</span></code>)</p></td>
<td><p>transformer+</p></td>
<td><p>Regular transformer processing a sequence of sub-word tokens. Before predicting spelling errors, sub-word representations within a word are aggregated and enriched with word features to obtain word representations. Predicts spelling errors on word level using those word representations.</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>sed_words (<code class="docutils literal notranslate"><span class="pre">nsed</span></code>/<code class="docutils literal notranslate"><span class="pre">SpellingErrorDetector</span></code>)</p></td>
<td><p>gnn</p></td>
<td><p>Attentional Graph Neural Network which processes language graphs with fully connected word nodes and fully connected sub-word cliques. Predicts spelling errors on word level using the word node representations.</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>sed_words (<code class="docutils literal notranslate"><span class="pre">nsed</span></code>/<code class="docutils literal notranslate"><span class="pre">SpellingErrorDetector</span></code>)</p></td>
<td><p>gnn+</p></td>
<td><p>Attentional Graph Neural Network which processes language graphs with fully connected word nodes, word features and fully connected sub-word cliques. Predicts spelling errors on word level using the word node representations.</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>sed_words (<code class="docutils literal notranslate"><span class="pre">nsed</span></code>/<code class="docutils literal notranslate"><span class="pre">SpellingErrorDetector</span></code>)</p></td>
<td><p>tokenization_repair+</p></td>
<td><p>Transformer based model that detects errors in sequences by first correcting the tokenizationand then detecting spelling errors for each word in the repaired text.</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>sed_words (<code class="docutils literal notranslate"><span class="pre">nsed</span></code>/<code class="docutils literal notranslate"><span class="pre">SpellingErrorDetector</span></code>)</p></td>
<td><p>tokenization_repair++</p></td>
<td><p>Transformer based model that detects errors in sequences by first correcting the tokenizationand then detecting spelling errors for each word in the repaired text. Different from tokenization_repair+ because this model was trained additionally to also correct spelling errors (it is also available in nsec).</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>sec (<code class="docutils literal notranslate"><span class="pre">nsec</span></code>/<code class="docutils literal notranslate"><span class="pre">SpellingErrorCorrector</span></code>)</p></td>
<td><p>transformer_words_nmt</p></td>
<td><p>Transformer model that corrects sequences by translating each word individually from misspelled to correct.</p></td>
<td><p>X</p></td>
</tr>
<tr class="row-even"><td><p>sec (<code class="docutils literal notranslate"><span class="pre">nsec</span></code>/<code class="docutils literal notranslate"><span class="pre">SpellingErrorCorrector</span></code>)</p></td>
<td><p>tokenization_repair++</p></td>
<td><p>Transformer based model that corrects sequences by first correcting the tokenization, then detecting spelling errors for each word in the repaired text and then translating every detected misspelled word to its corrected version.</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>sec (<code class="docutils literal notranslate"><span class="pre">nsec</span></code>/<code class="docutils literal notranslate"><span class="pre">SpellingErrorCorrector</span></code>)</p></td>
<td><p>transformer_nmt</p></td>
<td><p>Transformer model that translates a sequence with spelling errors into a sequence without spelling errors.</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>sec (<code class="docutils literal notranslate"><span class="pre">nsec</span></code>/<code class="docutils literal notranslate"><span class="pre">SpellingErrorCorrector</span></code>)</p></td>
<td><p>transformer_with_tokenization_repair_nmt</p></td>
<td><p>Transformer model that translates a sequence with spelling and tokenization errors into a sequence without spelling errors and tokenization errors. Different from transformer_nmt because this model tokenizes into characters and was trained on text with spelling and tokenization errors, whereas transformer_nmt tokenizes into sub-words and was trained only on text with spelling errors.</p></td>
<td></td>
</tr>
</tbody>
</table>
</section>
<span id="document-reproduce"></span><section id="training-and-reproducing-results">
<h1>Training and reproducing results<a class="headerlink" href="#training-and-reproducing-results" title="Permalink to this headline"></a></h1>
<section id="training">
<h2>Training<a class="headerlink" href="#training" title="Permalink to this headline"></a></h2>
<p>Before starting training you need the get the training data. Everything you need
(preprocessed samples, tokenizer, dictionaries, etc.) can be found under <code class="docutils literal notranslate"><span class="pre">/nfs/students/sebastian-walter/masters_thesis/data</span></code>.</p>
<p>You also need to set the following two special environment variables:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># set the data directory</span>
<span class="nb">export</span> <span class="nv">NSC_DATA_DIR</span><span class="o">=</span>/nfs/students/sebastian-walter/masters_thesis/data

<span class="c1"># set the config directory (necessary to be able to compose</span>
<span class="c1"># the final training config from sub-configs)</span>
<span class="nb">export</span> <span class="nv">NSC_CONFIG_DIR</span><span class="o">=</span>spell_checking/configs
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Of course you can also copy the training data folder to every other
place you like and adjust <code class="docutils literal notranslate"><span class="pre">NSC_DATA_DIR</span></code> accordingly. But keep in mind that this
folder is very large (&gt; 1TB).</p>
</div>
<p>After that you can train your own models using a training config.
All of the training configs this project used to train models can be found <a class="reference external" href="https://github.com/bastiscode/spell_check/tree/main/spell_checking/configs/train">here</a>.</p>
<p>You might have to further configure a training config by setting additional environment variables. Let’s
look at an example where we want to train a spelling error detection Graph Neural Network. The <a class="reference external" href="https://github.com/bastiscode/spell_check/tree/main/spell_checking/configs/train/sed_words.yaml">config
for this task</a> looks like the following:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">variant</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">${from_file:variant/sed_words.yaml}</span>
<span class="nt">model</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">${from_file:model/model_for_sed_words.yaml}</span>
<span class="nt">optimizer</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">${from_file:optimizer/adamw.yaml}</span>
<span class="nt">lr_scheduler</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">${from_file:lr_scheduler/step_with_0.05_warmup.yaml}</span>

<span class="nt">experiment_dir</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">${oc.env:NSC_EXPERIMENT_DIR}</span>
<span class="nt">data_dir</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">${oc.env:NSC_DATA_DIR}</span>
<span class="nt">datasets</span><span class="p">:</span>
  <span class="p p-Indicator">-</span> <span class="l l-Scalar l-Scalar-Plain">${oc.env:NSC_DATA_DIR}/processed/wikidump_paragraphs_sed_words_and_sec</span>
  <span class="p p-Indicator">-</span> <span class="l l-Scalar l-Scalar-Plain">${oc.env:NSC_DATA_DIR}/processed/bookcorpus_paragraphs_sed_words_and_sec</span>
<span class="nt">dataset_limits</span><span class="p">:</span>
  <span class="p p-Indicator">-</span> <span class="l l-Scalar l-Scalar-Plain">${oc.env:NSC_DATA_LIMIT}</span>
  <span class="p p-Indicator">-</span> <span class="l l-Scalar l-Scalar-Plain">${oc.env:NSC_DATA_LIMIT}</span>
<span class="nt">val_splits</span><span class="p">:</span>
  <span class="p p-Indicator">-</span> <span class="l l-Scalar l-Scalar-Plain">2500</span>
  <span class="p p-Indicator">-</span> <span class="l l-Scalar l-Scalar-Plain">2500</span>

<span class="nt">experiment_name</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">${oc.env:NSC_EXPERIMENT_NAME,dummy}</span>
<span class="nt">epochs</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">${oc.env:NSC_EPOCHS,1}</span>
<span class="nt">batch_max_length</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">${oc.env:NSC_BATCH_MAX_LENGTH,4096}</span>
<span class="nt">bucket_span</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">${oc.env:NSC_BUCKET_SPAN,4}</span>
<span class="nt">log_per_epoch</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">${oc.env:NSC_LOG_PER_EPOCH,100}</span>
<span class="nt">eval_per_epoch</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">${oc.env:NSC_EVAL_PER_EPOCH,4}</span>
<span class="nt">seed</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">22</span>
<span class="nt">mixed_precision</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">${oc.env:NSC_MIXED_PRECISION,true}</span>
</pre></div>
</div>
<p>Values looking like <code class="docutils literal notranslate"><span class="pre">${from_file:&lt;file_path&gt;}</span></code> refer to other config files relative to the <code class="docutils literal notranslate"><span class="pre">NSC_CONFIG_DIR</span></code>. When the training
config is composed, the contents of the referred config files will replace these values.</p>
<p>Values looking like <code class="docutils literal notranslate"><span class="pre">${oc.env:&lt;env_var_name&gt;,&lt;default&gt;}</span></code> refer to environment variables and an optional default that will be set
if the environment variable is not found. If there is no default you will be required to set the environment variable, otherwise
you receive an error message.</p>
<p>In our example we need to set values for the environment variables <code class="docutils literal notranslate"><span class="pre">NSC_DATA_LIMIT</span></code> (can be used to limit the number of samples per training dataset)
and <code class="docutils literal notranslate"><span class="pre">NSC_EXPERIMENT_DIR</span></code> (directory path where the logs and checkpoints will be saved). Once we have set these variables we
can start the training. Since the training script is written to support distributed training we need to use <a class="reference external" href="https://pytorch.org/docs/stable/elastic/run.html">torchrun</a>
to launch the script:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># set the environment variables</span>
<span class="nb">export</span> <span class="nv">NSC_DATA_LIMIT</span><span class="o">=</span><span class="m">100000</span> <span class="c1"># set data limit to 100,000 samples per dataset</span>
<span class="nb">export</span> <span class="nv">NSC_EXPERIMENT_DIR</span><span class="o">=</span>experiments <span class="c1"># directory path where the experiment will be saved</span>

<span class="c1"># to train locally / on a single node</span>
torchrun --nnodes<span class="o">=</span><span class="m">1</span> nsc/train.py --config spell_checking/configs/sed_words.yaml
</pre></div>
</div>
<p>You can also resume training for an existing experiment if you had to abort training for some reason:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># resume training from latest checkpoint of an experiment</span>
torchrun --nnodes<span class="o">=</span><span class="m">1</span> nsc/train.py --resume &lt;path_to_experiment_directory&gt;
</pre></div>
</div>
<p>As an alternative you can set one of the <code class="docutils literal notranslate"><span class="pre">NSC_CONFIG</span></code> or <code class="docutils literal notranslate"><span class="pre">NSC_RESUME</span></code> environment variables
and use the <a class="reference external" href="https://github.com/bastiscode/spell_check/tree/main/spell_checking/scripts/train.sh">train.sh</a> script to start training. This script additionally provides functionality to start distributed
training on <a class="reference external" href="https://slurm.schedmd.com/documentation.html">SLURM</a> clusters. Training using this script would look something like this:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># set the environment variables</span>
<span class="nb">export</span> <span class="nv">NSC_DATA_LIMIT</span><span class="o">=</span><span class="m">100000</span> <span class="c1"># set data limit to 100,000 samples per dataset</span>
<span class="nb">export</span> <span class="nv">NSC_EXPERIMENT_DIR</span><span class="o">=</span>experiments <span class="c1"># directory path where the experiment will be saved</span>

<span class="c1">## LOCAL training</span>
<span class="c1"># start new training run using a config</span>
<span class="nv">NSC_CONFIG</span><span class="o">=</span>spell_checking/configs/sed_words.yaml spell_checking/scripts/train.sh

<span class="c1"># resume training from latest checkpoint of an experiment</span>
<span class="nv">NSC_RESUME</span><span class="o">=</span>&lt;path_to_experiment_directory&gt; spell_checking/scripts/train.sh

<span class="c1">## SLURM training</span>
<span class="c1"># starting distributed training on a SLURM cluster using sbatch</span>
<span class="c1"># requires you to set the NSC_WORLD_SIZE environment variable (total number of GPUs used for training)</span>
<span class="c1"># if you e.g. want to train on 4 nodes with 2 GPUs each set NSC_WORLD_SIZE=8</span>
<span class="nv">NSC_CONFIG</span><span class="o">=</span>spell_checking/configs/sed_words.yaml <span class="nv">NSC_WORLD_SIZE</span><span class="o">=</span><span class="m">8</span> sbatch --nodes<span class="o">=</span><span class="m">4</span> --ntasks-per-node<span class="o">=</span><span class="m">2</span> --gres<span class="o">=</span>gpu:2 spell_checking/scripts/train.sh

<span class="c1"># if you are in an interactive SLURM session (started e.g. with srun)</span>
<span class="c1"># you probably want to train as if you are running locally, set NSC_FORCE_LOCAL=true and</span>
<span class="c1"># start training without sbatch</span>
<span class="nv">NSC_FORCE_LOCAL</span><span class="o">=</span><span class="nb">true</span> <span class="nv">NSC_CONFIG</span><span class="o">=</span>spell_checking/configs/sed_words.yaml spell_checking/scripts/train.sh
</pre></div>
</div>
</section>
<section id="reproduce">
<h2>Reproduce<a class="headerlink" href="#reproduce" title="Permalink to this headline"></a></h2>
<p>To reproduce the results of this project see the <code class="docutils literal notranslate"><span class="pre">train_slurm_&lt;task&gt;.sh</span></code> scripts in this <a class="reference external" href="https://github.com/bastiscode/spell_check/tree/main/spell_checking/scripts">directory</a> which were used for training all models.
These scripts do nothing more than setting some environment variables and calling the <code class="docutils literal notranslate"><span class="pre">train.sh</span></code> script mentioned above.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Using the <code class="docutils literal notranslate"><span class="pre">train_slurm_&lt;task&gt;.sh</span></code> scripts to reproduce results is only possible on a SLURM cluster
since they call the <code class="docutils literal notranslate"><span class="pre">train.sh</span></code> script using SLURMs sbatch command.</p>
</div>
<p>Once you finished training you can evaluate the models on the projects’ benchmarks that are available under
<code class="docutils literal notranslate"><span class="pre">/nfs/students/sebastian-walter/masters_thesis/benchmarks</span></code>.</p>
</section>
</section>
<span id="document-nsc"></span><section id="module-nsc">
<span id="nsc-package"></span><h1>nsc package<a class="headerlink" href="#module-nsc" title="Permalink to this headline"></a></h1>
<dl class="py class">
<dt class="sig sig-object py" id="nsc.BeamSearch">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">nsc.</span></span><span class="sig-name descname"><span class="pre">BeamSearch</span></span><a class="headerlink" href="#nsc.BeamSearch" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">nsc.api.sec.Search</span></code></p>
<p>Beam search: Keep the best beam_width paths during search.</p>
<dl class="py method">
<dt class="sig sig-object py" id="nsc.BeamSearch.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">beam_width</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">5</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#nsc.BeamSearch.__init__" title="Permalink to this definition"></a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>beam_width</strong> (<em>int</em>) – </p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>None</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="nsc.BeamSearch.beam_width">
<span class="sig-name descname"><span class="pre">beam_width</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="pre">int</span></em><em class="property"><span class="w">  </span><span class="p"><span class="pre">=</span></span><span class="w">  </span><span class="pre">5</span></em><a class="headerlink" href="#nsc.BeamSearch.beam_width" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="nsc.BestFirstSearch">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">nsc.</span></span><span class="sig-name descname"><span class="pre">BestFirstSearch</span></span><a class="headerlink" href="#nsc.BestFirstSearch" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">nsc.api.sec.Search</span></code></p>
<p>Best first search: Choose the highest scoring path out of all paths encountered so far.</p>
<dl class="py method">
<dt class="sig sig-object py" id="nsc.BestFirstSearch.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#nsc.BestFirstSearch.__init__" title="Permalink to this definition"></a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>None</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="nsc.GreedySearch">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">nsc.</span></span><span class="sig-name descname"><span class="pre">GreedySearch</span></span><a class="headerlink" href="#nsc.GreedySearch" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">nsc.api.sec.Search</span></code></p>
<p>Greedy search: Always take the path with the highest score.</p>
<dl class="py method">
<dt class="sig sig-object py" id="nsc.GreedySearch.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#nsc.GreedySearch.__init__" title="Permalink to this definition"></a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>None</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="nsc.SampleSearch">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">nsc.</span></span><span class="sig-name descname"><span class="pre">SampleSearch</span></span><a class="headerlink" href="#nsc.SampleSearch" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">nsc.api.sec.Search</span></code></p>
<p>Sample search: Choose a random path from the top_k highest scoring paths.</p>
<dl class="py method">
<dt class="sig sig-object py" id="nsc.SampleSearch.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">top_k</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">5</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#nsc.SampleSearch.__init__" title="Permalink to this definition"></a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>top_k</strong> (<em>int</em>) – </p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>None</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="nsc.SampleSearch.top_k">
<span class="sig-name descname"><span class="pre">top_k</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="pre">int</span></em><em class="property"><span class="w">  </span><span class="p"><span class="pre">=</span></span><span class="w">  </span><span class="pre">5</span></em><a class="headerlink" href="#nsc.SampleSearch.top_k" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="nsc.SpellingCorrectionScore">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">nsc.</span></span><span class="sig-name descname"><span class="pre">SpellingCorrectionScore</span></span><a class="headerlink" href="#nsc.SpellingCorrectionScore" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Determines how paths during decoding are scored.</p>
<p>The default mode <cite>log_likelihood</cite> is to score path using the
token sequence log likelihood given as the sum of all token log probabilities one got during
decoding a particular path normalized by the token sequence length (so shorter paths are not preferred):</p>
<blockquote>
<div><p>score = sum(log_probabilities) / (len(log_probabilities) ** alpha)</p>
</div></blockquote>
<p>Alpha here can be used to steer the decoding towards shorter or longer sequences, if alpha &gt; 1 longer sequences are
preferred, if alpha &lt; 1 shorter sequences are preferred.</p>
<p>Other supported modes are <cite>dictionary</cite>, <cite>dictionary_or_eq_input</cite> and <cite>dictionary_or_in_input</cite>. They only allow
paths that either contain dictionary words only, contain dictionary words or are equal to the input text or contain
dictionary words or words from the input text.
Note that for all these modes prefix_index must be specified, since we use a prefix index to determine if
a word is in a dictionary or is a prefix of a word in a dictionary.</p>
<dl class="py method">
<dt class="sig sig-object py" id="nsc.SpellingCorrectionScore.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">normalize_by_length</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'log_likelihood'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prefix_index</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#nsc.SpellingCorrectionScore.__init__" title="Permalink to this definition"></a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>normalize_by_length</strong> (<em>bool</em>) – </p></li>
<li><p><strong>alpha</strong> (<em>float</em>) – </p></li>
<li><p><strong>mode</strong> (<em>str</em>) – </p></li>
<li><p><strong>prefix_index</strong> (<em>Optional</em><em>[</em><em>nsc.data.index.PrefixIndex</em><em>]</em>) – </p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>None</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="nsc.SpellingCorrectionScore.alpha">
<span class="sig-name descname"><span class="pre">alpha</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="pre">float</span></em><em class="property"><span class="w">  </span><span class="p"><span class="pre">=</span></span><span class="w">  </span><span class="pre">1.0</span></em><a class="headerlink" href="#nsc.SpellingCorrectionScore.alpha" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="nsc.SpellingCorrectionScore.mode">
<span class="sig-name descname"><span class="pre">mode</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="pre">str</span></em><em class="property"><span class="w">  </span><span class="p"><span class="pre">=</span></span><span class="w">  </span><span class="pre">'log_likelihood'</span></em><a class="headerlink" href="#nsc.SpellingCorrectionScore.mode" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="nsc.SpellingCorrectionScore.normalize_by_length">
<span class="sig-name descname"><span class="pre">normalize_by_length</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="pre">bool</span></em><em class="property"><span class="w">  </span><span class="p"><span class="pre">=</span></span><span class="w">  </span><span class="pre">True</span></em><a class="headerlink" href="#nsc.SpellingCorrectionScore.normalize_by_length" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="nsc.SpellingCorrectionScore.prefix_index">
<span class="sig-name descname"><span class="pre">prefix_index</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">nsc.data.index.PrefixIndex</span><span class="p"><span class="pre">]</span></span></em><em class="property"><span class="w">  </span><span class="p"><span class="pre">=</span></span><span class="w">  </span><span class="pre">None</span></em><a class="headerlink" href="#nsc.SpellingCorrectionScore.prefix_index" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="nsc.SpellingErrorCorrector">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">nsc.</span></span><span class="sig-name descname"><span class="pre">SpellingErrorCorrector</span></span><a class="headerlink" href="#nsc.SpellingErrorCorrector" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">nsc.api.utils._APIBase</span></code></p>
<p>Spelling error correction</p>
<p>Class to run spelling error correction models.</p>
<dl class="py method">
<dt class="sig sig-object py" id="nsc.SpellingErrorCorrector.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model_dir</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#nsc.SpellingErrorCorrector.__init__" title="Permalink to this definition"></a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model_dir</strong> (<em>str</em>) – </p></li>
<li><p><strong>device</strong> (<em>Union</em><em>[</em><em>str</em><em>, </em><em>int</em><em>]</em>) – </p></li>
<li><p><strong>kwargs</strong> (<em>Dict</em><em>[</em><em>str</em><em>, </em><em>Any</em><em>]</em>) – </p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>None</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nsc.SpellingErrorCorrector.correct_file">
<span class="sig-name descname"><span class="pre">correct_file</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_file_path</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_file_path</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">detections</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">search</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">GreedySearch()</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">score</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">SpellingCorrectionScore(normalize_by_length=True,</span> <span class="pre">alpha=1.0,</span> <span class="pre">mode='log_likelihood',</span> <span class="pre">prefix_index=None)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">16</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_max_length_factor</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sort_by_length</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">show_progress</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#nsc.SpellingErrorCorrector.correct_file" title="Permalink to this definition"></a></dt>
<dd><p>Correct spelling errors in a file.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_file_path</strong> (<em>str</em>) – path to an input file, which will be corrected line by line</p></li>
<li><p><strong>output_file_path</strong> (<em>Optional</em><em>[</em><em>str</em><em>]</em>) – path to an output file, where corrected text will be saved line by line</p></li>
<li><p><strong>detections</strong> (<em>Optional</em><em>[</em><em>Union</em><em>[</em><em>str</em><em>, </em><em>List</em><em>[</em><em>List</em><em>[</em><em>int</em><em>]</em><em>]</em><em>]</em><em>]</em>) – spelling error detections (from a SpellingErrorDetector) to guide the correction, can either
be a path to a file containing detections or a list of lists of integers</p></li>
<li><p><strong>search</strong> (<em>nsc.api.sec.Search</em>) – instance of a Search object to determine the search method to use for decoding</p></li>
<li><p><strong>score</strong> (<a class="reference internal" href="index.html#nsc.SpellingCorrectionScore" title="nsc.api.sec.SpellingCorrectionScore"><em>nsc.api.sec.SpellingCorrectionScore</em></a>) – instance of a SpellingCorrectionScore object to determine how to score search paths during decoding</p></li>
<li><p><strong>batch_size</strong> (<em>int</em>) – how many sequences to process at once</p></li>
<li><p><strong>batch_max_length_factor</strong> (<em>Optional</em><em>[</em><em>float</em><em>]</em>) – sets the maximum total length of a batch to be
batch_max_length_factor * model_max_input_length, if a model e.g. has a max input length of 512 tokens
and batch_max_length_factor is 4 then one batch will contain as many input sequences as fit within
512 * 4 = 2048 tokens (takes precedence over batch_size if specified)</p></li>
<li><p><strong>sort_by_length</strong> (<em>bool</em>) – sort the inputs by length before processing them</p></li>
<li><p><strong>show_progress</strong> (<em>bool</em>) – display progress bar</p></li>
<li><p><strong>kwargs</strong> (<em>Any</em>) – </p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><em>Optional</em>[<em>List</em>[str]]</p>
</dd>
</dl>
<p>Returns: corrected file as list of strings if output_file_path is not specified else None</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nsc.SpellingErrorCorrector.correct_text">
<span class="sig-name descname"><span class="pre">correct_text</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">detections</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">search</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">GreedySearch()</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">score</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">SpellingCorrectionScore(normalize_by_length=True,</span> <span class="pre">alpha=1.0,</span> <span class="pre">mode='log_likelihood',</span> <span class="pre">prefix_index=None)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">16</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_max_length_factor</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sort_by_length</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">show_progress</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#nsc.SpellingErrorCorrector.correct_text" title="Permalink to this definition"></a></dt>
<dd><p>Correct spelling errors in text.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>inputs</strong> (<em>Union</em><em>[</em><em>str</em><em>, </em><em>List</em><em>[</em><em>str</em><em>]</em><em>]</em>) – text to correct given as a single string or a list of strings</p></li>
<li><p><strong>detections</strong> (<em>Optional</em><em>[</em><em>Union</em><em>[</em><em>List</em><em>[</em><em>int</em><em>]</em><em>, </em><em>List</em><em>[</em><em>List</em><em>[</em><em>int</em><em>]</em><em>]</em><em>]</em><em>]</em>) – spelling error detections (from a SpellingErrorDetector) to guide the correction, if
inputs is a single str, detections must be a list of integers, otherwise if inputs is a list of strings,
detections should be a list of lists of integers</p></li>
<li><p><strong>search</strong> (<em>nsc.api.sec.Search</em>) – Search instance to determine the search method to use for decoding</p></li>
<li><p><strong>score</strong> (<a class="reference internal" href="index.html#nsc.SpellingCorrectionScore" title="nsc.api.sec.SpellingCorrectionScore"><em>nsc.api.sec.SpellingCorrectionScore</em></a>) – SpellingCorrectionScore instance to determine how to score search paths during decoding</p></li>
<li><p><strong>batch_size</strong> (<em>int</em>) – how many sequences to process at once</p></li>
<li><p><strong>batch_max_length_factor</strong> (<em>Optional</em><em>[</em><em>float</em><em>]</em>) – sets the maximum total length of a batch to be
batch_max_length_factor * model_max_input_length, if a model e.g. has a max input length of 512 tokens
and batch_max_length_factor is 4 then one batch will contain as many input sequences as fit within
512 * 4 = 2048 tokens (takes precedence over batch_size if specified)</p></li>
<li><p><strong>sort_by_length</strong> (<em>bool</em>) – sort the inputs by length before processing them</p></li>
<li><p><strong>show_progress</strong> (<em>bool</em>) – display progress bar</p></li>
<li><p><strong>kwargs</strong> (<em>Any</em>) – </p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><em>Union</em>[str, <em>List</em>[str]]</p>
</dd>
</dl>
<p>Returns: corrected text as string or list of strings</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nsc.SpellingErrorCorrector.from_experiment">
<em class="property"><span class="pre">static</span><span class="w">  </span></em><span class="sig-name descname"><span class="pre">from_experiment</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">experiment_dir</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'cuda'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#nsc.SpellingErrorCorrector.from_experiment" title="Permalink to this definition"></a></dt>
<dd><p>Create a new NSC instance using your own experiment.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>experiment_dir</strong> (<em>str</em>) – path to the experiment directory</p></li>
<li><p><strong>device</strong> (<em>Union</em><em>[</em><em>str</em><em>, </em><em>int</em><em>]</em>) – device to load the model to (e.g. “cuda”, “cpu” or integer denoting GPU device index)</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><a class="reference internal" href="index.html#nsc.SpellingErrorCorrector" title="nsc.api.sec.SpellingErrorCorrector">nsc.api.sec.SpellingErrorCorrector</a></p>
</dd>
</dl>
<p>Returns: NSC instance</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nsc.SpellingErrorCorrector.from_pretrained">
<em class="property"><span class="pre">static</span><span class="w">  </span></em><span class="sig-name descname"><span class="pre">from_pretrained</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">task</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'sec'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'transformer_words_nmt'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'cuda'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cache_dir</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">force_download</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#nsc.SpellingErrorCorrector.from_pretrained" title="Permalink to this definition"></a></dt>
<dd><p>Create a new NSC instance using a pretrained model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>task</strong> (<em>str</em>) – name of the task</p></li>
<li><p><strong>model</strong> (<em>str</em>) – name of the pretrained model</p></li>
<li><p><strong>device</strong> (<em>Union</em><em>[</em><em>str</em><em>, </em><em>int</em><em>]</em>) – device to load the model to (e.g. “cuda”, “cpu” or integer denoting GPU device index)</p></li>
<li><p><strong>cache_dir</strong> (<em>Optional</em><em>[</em><em>str</em><em>]</em>) – local cache directory to store the pretrained model</p></li>
<li><p><strong>force_download</strong> (<em>bool</em>) – force download the pretrained model again even if it already exists in the cache_dir</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><a class="reference internal" href="index.html#nsc.SpellingErrorCorrector" title="nsc.api.sec.SpellingErrorCorrector">nsc.api.sec.SpellingErrorCorrector</a></p>
</dd>
</dl>
<p>Returns: NSC instance</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="nsc.SpellingErrorCorrector.mixed_precision_enabled">
<em class="property"><span class="pre">property</span><span class="w">  </span></em><span class="sig-name descname"><span class="pre">mixed_precision_enabled</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="pre">bool</span></em><a class="headerlink" href="#nsc.SpellingErrorCorrector.mixed_precision_enabled" title="Permalink to this definition"></a></dt>
<dd><p>Check if mixed precision is enabled (precision is set to something else than fp32).</p>
<p>Returns: bool whether mixed precision is enabled</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="nsc.SpellingErrorCorrector.model_name">
<em class="property"><span class="pre">property</span><span class="w">  </span></em><span class="sig-name descname"><span class="pre">model_name</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="pre">str</span></em><a class="headerlink" href="#nsc.SpellingErrorCorrector.model_name" title="Permalink to this definition"></a></dt>
<dd><p>Gives the name of the NSC model in use.</p>
<p>Returns: name of the model</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nsc.SpellingErrorCorrector.set_precision">
<span class="sig-name descname"><span class="pre">set_precision</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">precision</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#nsc.SpellingErrorCorrector.set_precision" title="Permalink to this definition"></a></dt>
<dd><p>Set the inference precision to use. Default is standard 32bit full precision.
Using 16bit floats or 16bit brain floats should only be used when you have a supported NVIDIA GPU.
When running on CPU fp16 will be overwritten with bfp16 since only bfp16 is supported on CPU for now.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>precision</strong> (<em>str</em>) – precision identifier (one of {fp32, fp16, bfp16})</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>None</p>
</dd>
</dl>
<p>Returns: None</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="nsc.SpellingErrorCorrector.task_name">
<em class="property"><span class="pre">property</span><span class="w">  </span></em><span class="sig-name descname"><span class="pre">task_name</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="pre">str</span></em><a class="headerlink" href="#nsc.SpellingErrorCorrector.task_name" title="Permalink to this definition"></a></dt>
<dd><p>Check which NSC task is run.</p>
<p>Returns: name of the task</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nsc.SpellingErrorCorrector.to">
<span class="sig-name descname"><span class="pre">to</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">device</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#nsc.SpellingErrorCorrector.to" title="Permalink to this definition"></a></dt>
<dd><p>Move the model to a different device.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>device</strong> (<em>Union</em><em>[</em><em>str</em><em>, </em><em>int</em><em>]</em>) – device specifier (e.g. “cuda”, “cpu” or integer denoting GPU device index)</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>nsc.api.utils._APIBase</p>
</dd>
</dl>
<p>Returns: self</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="nsc.SpellingErrorDetector">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">nsc.</span></span><span class="sig-name descname"><span class="pre">SpellingErrorDetector</span></span><a class="headerlink" href="#nsc.SpellingErrorDetector" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">nsc.api.utils._APIBase</span></code></p>
<p>Spelling error detection</p>
<p>Class to run spelling error detection models.</p>
<dl class="py method">
<dt class="sig sig-object py" id="nsc.SpellingErrorDetector.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model_dir</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#nsc.SpellingErrorDetector.__init__" title="Permalink to this definition"></a></dt>
<dd><p>Spelling error detection constructor.</p>
<p>Do not use this explicitly.
Use the static SpellingErrorDetector.from_pretrained() and SpellingErrorDetector.from_experiment() methods
instead.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model_dir</strong> (<em>str</em>) – directory of the model to load</p></li>
<li><p><strong>device</strong> (<em>Union</em><em>[</em><em>str</em><em>, </em><em>int</em><em>]</em>) – device to load the model in</p></li>
<li><p><strong>kwargs</strong> (<em>Dict</em><em>[</em><em>str</em><em>, </em><em>Any</em><em>]</em>) – </p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>None</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nsc.SpellingErrorDetector.detect_file">
<span class="sig-name descname"><span class="pre">detect_file</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_file_path</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_file_path</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">threshold</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">16</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_max_length_factor</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sort_by_length</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">show_progress</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#nsc.SpellingErrorDetector.detect_file" title="Permalink to this definition"></a></dt>
<dd><p>Detect spelling errors in a file.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_file_path</strong> (<em>str</em>) – path to an input file, which will be checked for spelling errors line by line</p></li>
<li><p><strong>output_file_path</strong> (<em>Optional</em><em>[</em><em>str</em><em>]</em>) – path to an output file, where the detections will be saved line by line</p></li>
<li><p><strong>threshold</strong> (<em>float</em>) – set detection threshold (0 &lt; threshold &lt; 1)</p></li>
<li><p><strong>batch_size</strong> (<em>int</em>) – how many sequences to process at once</p></li>
<li><p><strong>batch_max_length_factor</strong> (<em>Optional</em><em>[</em><em>float</em><em>]</em>) – sets the maximum total length of a batch to be
batch_max_length_factor * model_max_input_length, if a model e.g. has a max input length of 512 tokens
and batch_max_length_factor is 4 then one batch will contain as many input sequences as fit within
512 * 4 = 2048 tokens (takes precedence over batch_size if specified)</p></li>
<li><p><strong>sort_by_length</strong> (<em>bool</em>) – sort the inputs by length before processing them</p></li>
<li><p><strong>show_progress</strong> (<em>bool</em>) – display progress bar</p></li>
<li><p><strong>kwargs</strong> (<em>Any</em>) – </p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><em>Optional</em>[<em>Tuple</em>[<em>List</em>[<em>List</em>[int]], <em>List</em>[str]]]</p>
</dd>
</dl>
<dl class="simple">
<dt>Returns: tuple of detections as list of lists of integers and output strings as list of strings</dt><dd><p>if output_file_path is not specified else None</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nsc.SpellingErrorDetector.detect_text">
<span class="sig-name descname"><span class="pre">detect_text</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">threshold</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">16</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_max_length_factor</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sort_by_length</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">show_progress</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#nsc.SpellingErrorDetector.detect_text" title="Permalink to this definition"></a></dt>
<dd><p>Detect spelling errors in text.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>inputs</strong> (<em>Union</em><em>[</em><em>str</em><em>, </em><em>List</em><em>[</em><em>str</em><em>]</em><em>]</em>) – text to check for errors given as a single string or a list of strings</p></li>
<li><p><strong>threshold</strong> (<em>float</em>) – set detection threshold (0 &lt; threshold &lt; 1)</p></li>
<li><p><strong>batch_size</strong> (<em>int</em>) – how many sequences to process at once</p></li>
<li><p><strong>batch_max_length_factor</strong> (<em>Optional</em><em>[</em><em>float</em><em>]</em>) – sets the maximum total length of a batch to be
batch_max_length_factor * model_max_input_length, if a model e.g. has a max input length of 512 tokens
and batch_max_length_factor is 4 then one batch will contain as many input sequences as fit within
512 * 4 = 2048 tokens (takes precedence over batch_size if specified)</p></li>
<li><p><strong>sort_by_length</strong> (<em>bool</em>) – sort the inputs by length before processing them</p></li>
<li><p><strong>show_progress</strong> (<em>bool</em>) – display progress bar</p></li>
<li><p><strong>kwargs</strong> (<em>Any</em>) – </p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><em>Tuple</em>[<em>Union</em>[<em>List</em>[int], <em>List</em>[<em>List</em>[int]]], <em>Union</em>[str, <em>List</em>[str]]]</p>
</dd>
</dl>
<dl class="simple">
<dt>Returns: tuple of detections as list of integers or list of lists of integers and output strings as</dt><dd><p>str or list of strings</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nsc.SpellingErrorDetector.from_experiment">
<em class="property"><span class="pre">static</span><span class="w">  </span></em><span class="sig-name descname"><span class="pre">from_experiment</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">experiment_dir</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'cuda'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#nsc.SpellingErrorDetector.from_experiment" title="Permalink to this definition"></a></dt>
<dd><p>Create a new NSC instance using your own experiment.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>experiment_dir</strong> (<em>str</em>) – path to the experiment directory</p></li>
<li><p><strong>device</strong> (<em>Union</em><em>[</em><em>str</em><em>, </em><em>int</em><em>]</em>) – device to load the model to (e.g. “cuda”, “cpu” or integer denoting GPU device index)</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><a class="reference internal" href="index.html#nsc.SpellingErrorDetector" title="nsc.api.sed.SpellingErrorDetector">nsc.api.sed.SpellingErrorDetector</a></p>
</dd>
</dl>
<p>Returns: NSC instance</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nsc.SpellingErrorDetector.from_pretrained">
<em class="property"><span class="pre">static</span><span class="w">  </span></em><span class="sig-name descname"><span class="pre">from_pretrained</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">task</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'sed_words'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'gnn_default'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'cuda'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cache_dir</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">force_download</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#nsc.SpellingErrorDetector.from_pretrained" title="Permalink to this definition"></a></dt>
<dd><p>Create a new NSC instance using a pretrained model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>task</strong> (<em>str</em>) – name of the task</p></li>
<li><p><strong>model</strong> (<em>str</em>) – name of the pretrained model</p></li>
<li><p><strong>device</strong> (<em>Union</em><em>[</em><em>str</em><em>, </em><em>int</em><em>]</em>) – device to load the model to (e.g. “cuda”, “cpu” or integer denoting GPU device index)</p></li>
<li><p><strong>cache_dir</strong> (<em>Optional</em><em>[</em><em>str</em><em>]</em>) – local cache directory to store the pretrained model</p></li>
<li><p><strong>force_download</strong> (<em>bool</em>) – force download the pretrained model again even if it already exists in the cache_dir</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><a class="reference internal" href="index.html#nsc.SpellingErrorDetector" title="nsc.api.sed.SpellingErrorDetector">nsc.api.sed.SpellingErrorDetector</a></p>
</dd>
</dl>
<p>Returns: NSC instance</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="nsc.SpellingErrorDetector.mixed_precision_enabled">
<em class="property"><span class="pre">property</span><span class="w">  </span></em><span class="sig-name descname"><span class="pre">mixed_precision_enabled</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="pre">bool</span></em><a class="headerlink" href="#nsc.SpellingErrorDetector.mixed_precision_enabled" title="Permalink to this definition"></a></dt>
<dd><p>Check if mixed precision is enabled (precision is set to something else than fp32).</p>
<p>Returns: bool whether mixed precision is enabled</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="nsc.SpellingErrorDetector.model_name">
<em class="property"><span class="pre">property</span><span class="w">  </span></em><span class="sig-name descname"><span class="pre">model_name</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="pre">str</span></em><a class="headerlink" href="#nsc.SpellingErrorDetector.model_name" title="Permalink to this definition"></a></dt>
<dd><p>Gives the name of the NSC model in use.</p>
<p>Returns: name of the model</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nsc.SpellingErrorDetector.set_precision">
<span class="sig-name descname"><span class="pre">set_precision</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">precision</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#nsc.SpellingErrorDetector.set_precision" title="Permalink to this definition"></a></dt>
<dd><p>Set the inference precision to use. Default is standard 32bit full precision.
Using 16bit floats or 16bit brain floats should only be used when you have a supported NVIDIA GPU.
When running on CPU fp16 will be overwritten with bfp16 since only bfp16 is supported on CPU for now.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>precision</strong> (<em>str</em>) – precision identifier (one of {fp32, fp16, bfp16})</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>None</p>
</dd>
</dl>
<p>Returns: None</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="nsc.SpellingErrorDetector.task_name">
<em class="property"><span class="pre">property</span><span class="w">  </span></em><span class="sig-name descname"><span class="pre">task_name</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="pre">str</span></em><a class="headerlink" href="#nsc.SpellingErrorDetector.task_name" title="Permalink to this definition"></a></dt>
<dd><p>Check which NSC task is run.</p>
<p>Returns: name of the task</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nsc.SpellingErrorDetector.to">
<span class="sig-name descname"><span class="pre">to</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">device</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#nsc.SpellingErrorDetector.to" title="Permalink to this definition"></a></dt>
<dd><p>Move the model to a different device.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>device</strong> (<em>Union</em><em>[</em><em>str</em><em>, </em><em>int</em><em>]</em>) – device specifier (e.g. “cuda”, “cpu” or integer denoting GPU device index)</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>nsc.api.utils._APIBase</p>
</dd>
</dl>
<p>Returns: self</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="nsc.TokenizationRepairer">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">nsc.</span></span><span class="sig-name descname"><span class="pre">TokenizationRepairer</span></span><a class="headerlink" href="#nsc.TokenizationRepairer" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">nsc.api.utils._APIBase</span></code></p>
<p>Tokenization repair</p>
<p>Class to run tokenization repair models.</p>
<dl class="py method">
<dt class="sig sig-object py" id="nsc.TokenizationRepairer.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model_dir</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#nsc.TokenizationRepairer.__init__" title="Permalink to this definition"></a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model_dir</strong> (<em>str</em>) – </p></li>
<li><p><strong>device</strong> (<em>Union</em><em>[</em><em>str</em><em>, </em><em>int</em><em>]</em>) – </p></li>
<li><p><strong>kwargs</strong> (<em>Dict</em><em>[</em><em>str</em><em>, </em><em>Any</em><em>]</em>) – </p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>None</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nsc.TokenizationRepairer.from_experiment">
<em class="property"><span class="pre">static</span><span class="w">  </span></em><span class="sig-name descname"><span class="pre">from_experiment</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">experiment_dir</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'cuda'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#nsc.TokenizationRepairer.from_experiment" title="Permalink to this definition"></a></dt>
<dd><p>Create a new NSC instance using your own experiment.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>experiment_dir</strong> (<em>str</em>) – path to the experiment directory</p></li>
<li><p><strong>device</strong> (<em>Union</em><em>[</em><em>str</em><em>, </em><em>int</em><em>]</em>) – device to load the model to (e.g. “cuda”, “cpu” or integer denoting GPU device index)</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><a class="reference internal" href="index.html#nsc.TokenizationRepairer" title="nsc.api.tokenization_repair.TokenizationRepairer">nsc.api.tokenization_repair.TokenizationRepairer</a></p>
</dd>
</dl>
<p>Returns: NSC instance</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nsc.TokenizationRepairer.from_pretrained">
<em class="property"><span class="pre">static</span><span class="w">  </span></em><span class="sig-name descname"><span class="pre">from_pretrained</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">task</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'tokenization_repair'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'transformer_eo_large'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'cuda'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cache_dir</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">force_download</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#nsc.TokenizationRepairer.from_pretrained" title="Permalink to this definition"></a></dt>
<dd><p>Create a new NSC instance using a pretrained model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>task</strong> (<em>str</em>) – name of the task</p></li>
<li><p><strong>model</strong> (<em>str</em>) – name of the pretrained model</p></li>
<li><p><strong>device</strong> (<em>Union</em><em>[</em><em>str</em><em>, </em><em>int</em><em>]</em>) – device to load the model to (e.g. “cuda”, “cpu” or integer denoting GPU device index)</p></li>
<li><p><strong>cache_dir</strong> (<em>Optional</em><em>[</em><em>str</em><em>]</em>) – local cache directory to store the pretrained model</p></li>
<li><p><strong>force_download</strong> (<em>bool</em>) – force download the pretrained model again even if it already exists in the cache_dir</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><a class="reference internal" href="index.html#nsc.TokenizationRepairer" title="nsc.api.tokenization_repair.TokenizationRepairer">nsc.api.tokenization_repair.TokenizationRepairer</a></p>
</dd>
</dl>
<p>Returns: NSC instance</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="nsc.TokenizationRepairer.mixed_precision_enabled">
<em class="property"><span class="pre">property</span><span class="w">  </span></em><span class="sig-name descname"><span class="pre">mixed_precision_enabled</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="pre">bool</span></em><a class="headerlink" href="#nsc.TokenizationRepairer.mixed_precision_enabled" title="Permalink to this definition"></a></dt>
<dd><p>Check if mixed precision is enabled (precision is set to something else than fp32).</p>
<p>Returns: bool whether mixed precision is enabled</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="nsc.TokenizationRepairer.model_name">
<em class="property"><span class="pre">property</span><span class="w">  </span></em><span class="sig-name descname"><span class="pre">model_name</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="pre">str</span></em><a class="headerlink" href="#nsc.TokenizationRepairer.model_name" title="Permalink to this definition"></a></dt>
<dd><p>Gives the name of the NSC model in use.</p>
<p>Returns: name of the model</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nsc.TokenizationRepairer.repair_file">
<span class="sig-name descname"><span class="pre">repair_file</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_file_path</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_file_path</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">16</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_max_length_factor</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sort_by_length</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">show_progress</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#nsc.TokenizationRepairer.repair_file" title="Permalink to this definition"></a></dt>
<dd><p>Repair whitespaces in a file.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_file_path</strong> (<em>str</em>) – path to an input file, which will be repaired line by line</p></li>
<li><p><strong>output_file_path</strong> (<em>Optional</em><em>[</em><em>str</em><em>]</em>) – path to an output file, where repaired text will be saved line by line</p></li>
<li><p><strong>batch_size</strong> (<em>int</em>) – how many sequences to process at once</p></li>
<li><p><strong>batch_max_length_factor</strong> (<em>Optional</em><em>[</em><em>float</em><em>]</em>) – sets the maximum total length of a batch to be
batch_max_length_factor * model_max_input_length, if a model e.g. has a max input length of 512 tokens
and batch_max_length_factor is 4 then one batch will contain as many input sequences as fit within
512 * 4 = 2048 tokens (takes precedence over batch_size if specified)</p></li>
<li><p><strong>sort_by_length</strong> (<em>bool</em>) – sort the inputs by length before processing them</p></li>
<li><p><strong>show_progress</strong> (<em>bool</em>) – display progress bar</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><em>Optional</em>[<em>Union</em>[<em>List</em>[int], <em>List</em>[<em>List</em>[int]]]]</p>
</dd>
</dl>
<p>Returns: repaired file as list of strings if output_file_path is not specified else None</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nsc.TokenizationRepairer.repair_text">
<span class="sig-name descname"><span class="pre">repair_text</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">16</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_max_length_factor</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sort_by_length</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">show_progress</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#nsc.TokenizationRepairer.repair_text" title="Permalink to this definition"></a></dt>
<dd><p>Repair whitespaces in text.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>inputs</strong> (<em>Union</em><em>[</em><em>str</em><em>, </em><em>List</em><em>[</em><em>str</em><em>]</em><em>]</em>) – text to repair given as a single string or a list of strings</p></li>
<li><p><strong>batch_size</strong> (<em>int</em>) – how many sequences to process at once</p></li>
<li><p><strong>batch_max_length_factor</strong> (<em>Optional</em><em>[</em><em>float</em><em>]</em>) – sets the maximum total length of a batch to be
batch_max_length_factor * model_max_input_length, if a model e.g. has a max input length of 512 tokens
and batch_max_length_factor is 4 then one batch will contain as many input sequences as fit within
512 * 4 = 2048 tokens (takes precedence over batch_size if specified)</p></li>
<li><p><strong>sort_by_length</strong> (<em>bool</em>) – sort the inputs by length before processing them</p></li>
<li><p><strong>show_progress</strong> (<em>bool</em>) – display progress bar</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><em>Union</em>[<em>List</em>[int], <em>List</em>[<em>List</em>[int]]]</p>
</dd>
</dl>
<p>Returns: repaired text as string or list of strings</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nsc.TokenizationRepairer.set_precision">
<span class="sig-name descname"><span class="pre">set_precision</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">precision</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#nsc.TokenizationRepairer.set_precision" title="Permalink to this definition"></a></dt>
<dd><p>Set the inference precision to use. Default is standard 32bit full precision.
Using 16bit floats or 16bit brain floats should only be used when you have a supported NVIDIA GPU.
When running on CPU fp16 will be overwritten with bfp16 since only bfp16 is supported on CPU for now.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>precision</strong> (<em>str</em>) – precision identifier (one of {fp32, fp16, bfp16})</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>None</p>
</dd>
</dl>
<p>Returns: None</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="nsc.TokenizationRepairer.task_name">
<em class="property"><span class="pre">property</span><span class="w">  </span></em><span class="sig-name descname"><span class="pre">task_name</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="pre">str</span></em><a class="headerlink" href="#nsc.TokenizationRepairer.task_name" title="Permalink to this definition"></a></dt>
<dd><p>Check which NSC task is run.</p>
<p>Returns: name of the task</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nsc.TokenizationRepairer.to">
<span class="sig-name descname"><span class="pre">to</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">device</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#nsc.TokenizationRepairer.to" title="Permalink to this definition"></a></dt>
<dd><p>Move the model to a different device.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>device</strong> (<em>Union</em><em>[</em><em>str</em><em>, </em><em>int</em><em>]</em>) – device specifier (e.g. “cuda”, “cpu” or integer denoting GPU device index)</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>nsc.api.utils._APIBase</p>
</dd>
</dl>
<p>Returns: self</p>
</dd></dl>

</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="nsc.get_available_spelling_error_correction_models">
<span class="sig-prename descclassname"><span class="pre">nsc.</span></span><span class="sig-name descname"><span class="pre">get_available_spelling_error_correction_models</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#nsc.get_available_spelling_error_correction_models" title="Permalink to this definition"></a></dt>
<dd><p>Get available spelling error correction models</p>
<p>Returns: list of spelling error correction model infos each containing the task name,
model name and a short description</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><em>List</em>[nsc.api.utils.ModelInfo]</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="nsc.get_available_spelling_error_detection_models">
<span class="sig-prename descclassname"><span class="pre">nsc.</span></span><span class="sig-name descname"><span class="pre">get_available_spelling_error_detection_models</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#nsc.get_available_spelling_error_detection_models" title="Permalink to this definition"></a></dt>
<dd><p>Get available spelling error detection models</p>
<p>Returns: list of spelling error detection model infos each containing the task name,
model name and a short description</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><em>List</em>[nsc.api.utils.ModelInfo]</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="nsc.get_available_tokenization_repair_models">
<span class="sig-prename descclassname"><span class="pre">nsc.</span></span><span class="sig-name descname"><span class="pre">get_available_tokenization_repair_models</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#nsc.get_available_tokenization_repair_models" title="Permalink to this definition"></a></dt>
<dd><p>Get available tokenization repair models</p>
<p>Returns: list of tokenization repair model infos each containing the task name, model name and a short description</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><em>List</em>[nsc.api.utils.ModelInfo]</p>
</dd>
</dl>
</dd></dl>

</section>
</div>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, Sebastian Walter.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>