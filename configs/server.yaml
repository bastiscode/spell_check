host: "0.0.0.0"
port: 12345
precision: "fp32"
timeout: 10.0
# >= 3 is recommended such that at least a full pipeline can be run on one GPU without moving models
max_models_per_gpu: 3
models:
  tokenization repair:
    - tokenization repair+
  sed words:
    - gnn+
  sec:
    - transformer nmt
    - transformer words nmt
    - transformer with tokenization repair nmt
