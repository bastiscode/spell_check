variant: ${from_file:variant/tokenization_repair_nmt.yaml}
model: ${from_file:model/model_for_tokenization_repair_nmt.yaml}
optimizer: ${from_file:optimizer/adamw.yaml}
lr_scheduler: ${from_file:lr_scheduler/cosine_with_0.05_warmup.yaml}

experiment_dir: ${oc.env:GNN_LIB_EXPERIMENT_DIR}
data_dir: ${oc.env:GNN_LIB_DATA_DIR}
datasets: [WIKIDUMP, BOOKCORPUS]

experiment_name: ${oc.env:GNN_LIB_EXPERIMENT_NAME,dummy}
epochs: ${oc.env:GNN_LIB_EPOCHS,1}
batch_max_length: ${oc.env:GNN_LIB_BATCH_MAX_LENGTH,4096}
bucket_span: ${oc.env:GNN_LIB_BUCKET_SPAN,4}
preprocess_name: ${oc.env:GNN_LIB_PREPROCESS_NAME,null}
limit_train_dataset: ${oc.env:GNN_LIB_TRAIN_LIMIT,10000000}
limit_val_dataset: ${oc.env:GNN_LIB_VAL_LIMIT,5000}
log_per_epoch: ${oc.env:GNN_LIB_LOG_PER_EPOCH,100}
eval_per_epoch: ${oc.env:GNN_LIB_EVAL_PER_EPOCH,4}
seed: 22
num_workers: ${oc.env:GNN_LIB_NUM_WORKERS,3}
exponential_moving_average: [0.99, 5]
