.PHONY: download_data
download_data:
	mkdir -p ../data/downloads

	@echo "Downloading latest Wikidump"
	@echo "(Cannot download Wikidump 2020/10/20 as used in the masters thesis since it is no longer available, using latest instead)"

	if [ ! -f ../data/downloads/enwiki-latest-pages-articles-multistream.xml.bz2 ] ; \
		then \
		wget https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles-multistream.xml.bz2 \
		-P ../data/downloads ; \
	fi;

	mkdir -p ../data/raw/wikidump
	python -m wikiextractor.WikiExtractor ../data/downloads/enwiki-latest-pages-articles-multistream.xml.bz2 \
		-o ../data/raw/wikidump

	@echo "Downloading Bookcorpus"

	if [ ! -f ../data/downloads/books1.tar.gz ] ; \
		then \
		wget https://battle.shawwn.com/sdb/books1/books1.tar.gz -P ../data/downloads ; \
	fi;

	mkdir -p ../data/raw/bookcorpus
	tar -xzvf ../data/downloads/books1.tar.gz -C ../data/raw/bookcorpus

.PHONY: download_misspellings
download_misspellings:
	@echo "Downloading misspellings"
	mkdir -p ../data/misspellings
	mkdir -p ../data/misspellings/birkbeck
	wget https://www.dcs.bbk.ac.uk/~ROGER/missp.dat -O ../data/misspellings/birkbeck/missp.dat
	wget https://www.dcs.bbk.ac.uk/~ROGER/holbrook-missp.dat -O ../data/misspellings/birkbeck/holbrook-missp.dat
	wget https://www.dcs.bbk.ac.uk/~ROGER/aspell.dat -O ../data/misspellings/birkbeck/aspell.dat
	wget https://www.dcs.bbk.ac.uk/~ROGER/wikipedia.dat -O ../data/misspellings/birkbeck/wikipedia.dat

	mkdir -p ../data/misspellings/toefl_spell
	wget https://github.com/EducationalTestingService/TOEFL-Spell/blob/master/Annotations.tsv \
	-O ../data/misspellings/toefl_spell/Annotations.tsv

	wget https://github.com/facebookresearch/moe/raw/master/data/moe_misspellings_train.zip \
	-O ../data/misspellings/moe.zip
	unzip ../data/misspellings/moe.zip -d ../data/misspellings/moe

.PHONY: setup_datasets
setup_datasets:
	@echo "Setting up Wikidump"
	python scripts/prepare_wikidump.py
	@echo "Setting up Bookcorpus"
	python scripts/prepare_bookcorpus.py
	@echo "Setting up Arxiv"
	python scripts/prepare_arxiv.py

.PHONY: setup_misspellings
setup_misspellings:
	python scripts/process_misspellings.py \
	--misspellings-dir ../data/raw/misspellings \
	--data-dir ../data \
	--dictionary ../data/dictionaries/merged_train_100k.txt \
	--out-dir ../data/misspellings

	python scripts/process_misspellings.py \
	--misspellings-dir ../data/raw/misspellings \
	--data-dir ../data \
	--dictionary ../data/dictionaries/merged_train_100k.txt \
	--out-dir ../data/misspellings_no_neuspell_no_bea \
	--no-neuspell --no-bea

.PHONY: setup_tokenizers
setup_tokenizers:
	@echo "Training tokenizers"
	python ../nsc/scripts/train_tokenizer.py --tokenizer BPE --vocab-size 1000 \
	--in-file ../data/cleaned/wikidump/train_files.txt ../data/cleaned/bookcorpus/train_files.txt \
	--out-file ../data/tokenizers/bpe/wiki_bookcorpus_1k_no_prefix_space.pkl \
	--max-sequences 20000000

	python ../nsc/scripts/train_tokenizer.py --tokenizer BPE --vocab-size 10000 \
	--in-file ../data/cleaned/wikidump/train_files.txt ../data/cleaned/bookcorpus/train_files.txt \
	--out-file ../data/tokenizers/bpe/wiki_bookcorpus_10k_no_prefix_space.pkl \
	--max-sequences 20000000

.PHONY: setup_dictionaries
setup_dictionaries:
	@echo "Creating Wikidump dictionary"
	python ../nsc/scripts/create_dictionary.py --in-file ../data/cleaned/wikidump/train_files.txt \
	--out-file ../data/dictionaries/wikidump_train_100k.txt --top-k 100000 --max-sequences 20000000

	@echo "Creating Bookcorpus dictionary"
	python ../nsc/scripts/create_dictionary.py --in-file ../data/cleaned/bookcorpus/train_files.txt \
	--out-file ../data/dictionaries/bookcorpus_train_100k.txt --top-k 100000 --max-sequences 20000000

	@echo "Creating merged dictionaries"
	python ../nsc/scripts/merge_dictionaries.py --dictionaries ../data/dictionaries/*_train_100k.txt \
	--out-file ../data/dictionaries/merged_train_100k.txt --min-freq 0

.PHONY: setup_spell_check_index
setup_spell_check_index:
	cd .. && \
	python nsc/scripts/create_spell_check_index.py \
	--in-file data/cleaned/wikidump/train_files.txt data/cleaned/bookcorpus/train_files.txt \
	--context-length 0 \
	--out-dir data/spell_check_index/ctx_0_ned_string \
	--dist norm_edit_distance \
	--vectorizer string \
	--dictionary-file data/dictionaries/merged_train_100k.txt \
	--max-files 2000

.PHONY: setup_vectorizer
setup_vectorizer:
	@bash scripts/setup_vectorizer.sh

.PHONY: setup_data
setup_data: setup_datasets setup_tokenizers setup_dictionaries setup_misspellings \
	setup_vectorizer setup_spell_check_index
