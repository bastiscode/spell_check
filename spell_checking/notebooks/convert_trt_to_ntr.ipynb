{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f816b83c-f361-4333-a17e-c37caec4a894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "186a8b54-3806-4c16-9e27-ff96552063c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import omegaconf\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from nsc.models import ModelForTokenClassification, ModelForTokenClassificationConfig, Models\n",
    "from nsc.modules import embedding\n",
    "from nsc import TokenizationRepairer as NSCTokenizationRepairer\n",
    "from nsc.data import tokenization, variants\n",
    "from nsc.utils import Batch, io\n",
    "from nsc.utils import config\n",
    "\n",
    "from spell_checking import CONFIG_DIR, DATA_DIR, EXPERIMENT_DIR\n",
    "\n",
    "from trt import TokenizationRepairer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ec4e390c-1a8e-4531-80db-29aaa7c6e236",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_tok_cfg = tokenization.TokenizerConfig(type=tokenization.Tokenizers.CHAR)\n",
    "char_tok = tokenization.get_tokenizer_from_config(char_tok_cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe056257-98a5-4a1a-8d4e-8d30a4b810d7",
   "metadata": {
    "tags": []
   },
   "source": [
    "### EO medium (6 layer transformer encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a08a6589-ba54-4048-86c7-b5aad6b8729b",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_cfg = embedding.TensorEmbeddingConfig()\n",
    "model_cfg = ModelForTokenClassificationConfig(\n",
    "    type=Models.MODEL_FOR_TOKEN_CLASSIFICATION, \n",
    "    hidden_dim=512,\n",
    "    num_classes=3,\n",
    "    tokenizer=char_tok_cfg,\n",
    "    embedding=embedding_cfg,\n",
    "    num_layers=6,\n",
    "    activation=\"gelu\",\n",
    "    feed_forward_dim=2048,\n",
    "    norm=False,\n",
    "    num_clf_layers=1\n",
    ")\n",
    "sample_inputs = Batch([torch.tensor([0, 1])], {})\n",
    "nsc_model = ModelForTokenClassification(sample_inputs=sample_inputs, cfg=model_cfg, device=\"cpu\").eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e6d4c1f5-e7e8-48bd-887c-046867869f28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': {'type': <Models.MODEL_FOR_TOKEN_CLASSIFICATION: 5>, 'max_length': 512, 'hidden_dim': 512, 'tokenizer': {'type': <Tokenizers.CHAR: 1>, 'file_path': None}, 'embedding': {'learned_position_embedding': False, 'embed_positions': True, 'dropout': 0.1}, 'dropout': 0.1, 'num_layers': 6, 'feed_forward_dim': 2048, 'norm': False, 'activation': 'gelu', 'num_clf_layers': 1, 'num_classes': 3}, 'variant': {'type': <DatasetVariants.TOKENIZATION_REPAIR: 3>, 'data_scheme': 'tensor', 'input_type': 'char', 'add_bos_eos': True}, 'experiment_dir': '???', 'experiment_name': 'eo_medium_arxiv_with_errors', 'data_dir': '???', 'datasets': '???', 'dataset_limits': '???', 'val_splits': '???', 'epochs': 20, 'batch_size': 64, 'batch_max_length': None, 'bucket_span': None, 'optimizer': '???', 'lr_scheduler': None, 'log_per_epoch': 100, 'eval_per_epoch': 4, 'keep_last_n_checkpoints': 0, 'seed': 22, 'num_workers': None, 'pin_memory': True, 'mixed_precision': True, 'start_from_checkpoint': None, 'exponential_moving_average': None}\n"
     ]
    }
   ],
   "source": [
    "variant_cfg = variants.TokenizationRepairConfig(\n",
    "    type=variants.DatasetVariants.TOKENIZATION_REPAIR,\n",
    "    data_scheme=\"tensor\",\n",
    "    input_type=\"char\",\n",
    "    add_bos_eos=True\n",
    ")\n",
    "dummy_train_cfg = OmegaConf.structured(\n",
    "    config.TrainConfig(\n",
    "        experiment_name=\"eo_medium_arxiv_with_errors\",\n",
    "        variant=variant_cfg,\n",
    "        model=model_cfg\n",
    "    )\n",
    ")\n",
    "print(dummy_train_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "645efb87-7322-4459-bf19-954cd8e23655",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-10 22:05:40,229 [DOWNLOAD] [INFO] model eo_medium_arxiv_with_errors was already downloaded to cache directory /home/sebastian/anaconda3/envs/masters_thesis/lib/python3.8/site-packages/trt/api/.cache\n",
      "2022-05-10 22:05:40,248 [TOKENIZATION_REPAIR] [INFO] running tokenization repair on device Intel(R) Core(TM) i7-9750H CPU @ 2.60GHz\n"
     ]
    }
   ],
   "source": [
    "tok_rep = TokenizationRepairer.from_pretrained(\"eo_medium_arxiv_with_errors\", device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ae166b36-b639-473e-8e3b-e59a51784402",
   "metadata": {},
   "outputs": [],
   "source": [
    "trt_tok = tok_rep.model.encoder.tokenizer\n",
    "trt_chars = [trt_tok.id_to_token(i) for i in range(trt_tok.get_vocab_size())]\n",
    "trt_tok.get_vocab_size(), trt_chars\n",
    "assert all(char in trt_chars for char in char_tok.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "65f9383c-97f7-4765-af67-58054fc43e00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<mask>', '<sep>'}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(trt_tok.get_vocab()) - set(char_tok.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b9eff31d-758e-4fd4-9506-01c856f825e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.0061,  0.0099,  0.0760,  ..., -0.0008, -0.0288,  0.0184],\n",
      "        [-0.0934,  0.0072,  0.0519,  ...,  0.0433,  0.0365,  0.0169],\n",
      "        [ 0.0578, -0.0113, -0.0313,  ...,  0.0502,  0.0699, -0.0762],\n",
      "        ...,\n",
      "        [-0.0378,  0.0389, -0.0255,  ...,  0.0730,  0.0513, -0.0658],\n",
      "        [ 0.0249,  0.0182,  0.0039,  ..., -0.0570,  0.0972, -0.0442],\n",
      "        [ 0.0150,  0.0171, -0.0260,  ...,  0.0048, -0.0491, -0.1052]],\n",
      "       requires_grad=True)\n",
      "torch.Size([99, 512]) tensor([[ 0.0091,  0.0069,  0.0073,  ..., -0.0129, -0.0797,  0.0664],\n",
      "        [ 0.0047, -0.0299,  0.0066,  ..., -0.0620, -0.0542, -0.0682],\n",
      "        [ 0.0097,  0.0094,  0.0074,  ..., -0.1044, -0.0106, -0.0480],\n",
      "        ...,\n",
      "        [ 0.0117,  0.0090,  0.0086,  ..., -0.0296, -0.1163,  0.0599],\n",
      "        [ 0.0083,  0.0050,  0.0066,  ..., -0.0551,  0.0331,  0.0179],\n",
      "        [ 0.0117,  0.0105,  0.0106,  ..., -0.0245, -0.0246, -0.0273]])\n"
     ]
    }
   ],
   "source": [
    "print(nsc_model.embedding.embedding.emb.weight)\n",
    "new_embedding_weight = torch.stack([\n",
    "    tok_rep.model.encoder.embedding.embedding.weight[trt_tok.token_to_id(char_tok.id_to_token(i))]\n",
    "    for i in range(char_tok.vocab_size)\n",
    "])\n",
    "print(new_embedding_weight.shape, new_embedding_weight)\n",
    "assert all(torch.equal(new_embedding_weight[char_tok.token_to_id(char)], tok_rep.model.encoder.embedding.embedding.weight[trt_tok.token_to_id(char)]) \n",
    "           for char in char_tok.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "60adb8bd-8463-4e58-b6be-62bb5bdbb0b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nsc_model.embedding.embedding.emb.weight.data = new_embedding_weight\n",
    "nsc_model.embedding.norm.load_state_dict(tok_rep.model.encoder.embedding.norm.state_dict())\n",
    "nsc_model.encoder.encoder.load_state_dict(tok_rep.model.encoder.encoder.state_dict())\n",
    "nsc_model.head.clf.load_state_dict(tok_rep.model.head.head.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "817105d2-d2da-47b8-b281-3505bc483493",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = \"thisisatest\"\n",
    "positions = torch.arange(len(\"thisisatest\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6a05b973-6268-45de-b2b2-313b92954181",
   "metadata": {},
   "outputs": [],
   "source": [
    "nsc_token_emb = nsc_model.embedding.embedding(torch.tensor(char_tok.tokenize(seq)))\n",
    "nsc_pos_emb = nsc_model.embedding.pos_emb(positions)\n",
    "nsc_emb = nsc_model.embedding(torch.tensor(char_tok.tokenize(seq)).unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2178282f-ba9f-4a23-b556-fa399ab5c782",
   "metadata": {},
   "outputs": [],
   "source": [
    "ipt_ids = torch.tensor([trt_tok.encode(seq).ids[1:-1]])\n",
    "trt_token_emb = tok_rep.model.encoder.embedding.embedding(ipt_ids.T)\n",
    "trt_pos_emb = tok_rep.model.encoder.embedding.pos_embedding(ipt_ids.T)\n",
    "trt_emb = tok_rep.model.encoder.embedding(ipt_ids.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5839000f-9aa2-4713-b940-2c6d07a4b70e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([11, 512]), torch.Size([11, 1, 512]))"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nsc_token_emb.shape, trt_pos_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "10e7843f-13a4-4799-950f-94737740084f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trt_all_emb = tok_rep.model.encoder.embedding.norm(trt_token_emb * 512 ** 0.5 + trt_pos_emb)\n",
    "nsc_all_emb = nsc_model.embedding.norm(nsc_token_emb + nsc_pos_emb)\n",
    "assert torch.allclose(trt_all_emb[:, 0, :], nsc_all_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5177f632-d0ea-4db9-9ecc-4f911fd752d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True, True)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "    torch.allclose(nsc_token_emb, trt_token_emb[:, 0, :] * 512 ** 0.5), \n",
    "    torch.allclose(nsc_pos_emb, trt_pos_emb[:, 0, :]), \n",
    "    torch.allclose(nsc_emb[0], trt_emb[:, 0, :])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "10e9a5c2-82c4-43d8-bcca-bc4bf14bb0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "nsc_enc = nsc_model.encoder(nsc_emb)\n",
    "trt_enc = tok_rep.model.encoder.encoder(trt_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "05c0c40b-45f6-44a9-b692-2bb4ead2e9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.allclose(nsc_enc[0], trt_enc[:, 0, :], atol=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "3de70bd8-e337-4f0f-b781-40306071cea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(os.path.join(EXPERIMENT_DIR, \"TOKENIZATION_REPAIR\", \"eo_medium_arxiv_with_errors_ported\", \"checkpoints\"), exist_ok=True)\n",
    "io.save_checkpoint(os.path.join(EXPERIMENT_DIR, \"TOKENIZATION_REPAIR\", \"eo_medium_arxiv_with_errors_ported\", \"checkpoints\", \"checkpoint_best.pt\"), \n",
    "                   nsc_model, 0, 0)\n",
    "with open(os.path.join(EXPERIMENT_DIR, \"TOKENIZATION_REPAIR\", \"eo_medium_arxiv_with_errors_ported\", \"cfg.pkl\"), \"wb\") as pf:\n",
    "    pickle.dump((dummy_train_cfg, {}), pf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78976720-bbe9-4d35-b61c-d504f13ff49e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### EO large (12 layer transformer encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9246668a-9469-4f3d-9a57-1902b591339e",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_cfg = embedding.TensorEmbeddingConfig()\n",
    "model_cfg = ModelForTokenClassificationConfig(\n",
    "    type=Models.MODEL_FOR_TOKEN_CLASSIFICATION, \n",
    "    hidden_dim=512,\n",
    "    num_classes=3,\n",
    "    tokenizer=char_tok_cfg,\n",
    "    embedding=embedding_cfg,\n",
    "    num_layers=12,\n",
    "    activation=\"gelu\",\n",
    "    feed_forward_dim=2048,\n",
    "    norm=False,\n",
    "    num_clf_layers=1\n",
    ")\n",
    "sample_inputs = Batch([torch.tensor([0, 1])], {})\n",
    "nsc_model = ModelForTokenClassification(sample_inputs=sample_inputs, cfg=model_cfg, device=\"cpu\").eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ee412a86-d09c-48f6-ad49-a28d9f2ddb21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': {'type': <Models.MODEL_FOR_TOKEN_CLASSIFICATION: 5>, 'max_length': 512, 'hidden_dim': 512, 'tokenizer': {'type': <Tokenizers.CHAR: 1>, 'file_path': None}, 'embedding': {'learned_position_embedding': False, 'embed_positions': True, 'dropout': 0.1}, 'dropout': 0.1, 'num_layers': 12, 'feed_forward_dim': 2048, 'norm': False, 'activation': 'gelu', 'num_clf_layers': 1, 'num_classes': 3}, 'variant': {'type': <DatasetVariants.TOKENIZATION_REPAIR: 3>, 'data_scheme': 'tensor', 'input_type': 'char', 'add_bos_eos': True}, 'experiment_dir': '???', 'experiment_name': 'eo_large_arxiv_with_errors', 'data_dir': '???', 'datasets': '???', 'dataset_limits': '???', 'val_splits': '???', 'epochs': 20, 'batch_size': 64, 'batch_max_length': None, 'bucket_span': None, 'optimizer': '???', 'lr_scheduler': None, 'log_per_epoch': 100, 'eval_per_epoch': 4, 'keep_last_n_checkpoints': 0, 'seed': 22, 'num_workers': None, 'pin_memory': True, 'mixed_precision': True, 'start_from_checkpoint': None, 'exponential_moving_average': None}\n"
     ]
    }
   ],
   "source": [
    "variant_cfg = variants.TokenizationRepairConfig(\n",
    "    type=variants.DatasetVariants.TOKENIZATION_REPAIR,\n",
    "    data_scheme=\"tensor\",\n",
    "    input_type=\"char\",\n",
    "    add_bos_eos=True\n",
    ")\n",
    "dummy_train_cfg = OmegaConf.structured(\n",
    "    config.TrainConfig(\n",
    "        experiment_name=\"eo_large_arxiv_with_errors\",\n",
    "        variant=variant_cfg,\n",
    "        model=model_cfg\n",
    "    )\n",
    ")\n",
    "print(dummy_train_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4f30a728-1388-4957-9ae9-e9384748f612",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-10 22:06:24,297 [DOWNLOAD] [INFO] model eo_large_arxiv_with_errors was already downloaded to cache directory /home/sebastian/anaconda3/envs/masters_thesis/lib/python3.8/site-packages/trt/api/.cache\n",
      "2022-05-10 22:06:24,316 [TOKENIZATION_REPAIR] [INFO] running tokenization repair on device Intel(R) Core(TM) i7-9750H CPU @ 2.60GHz\n"
     ]
    }
   ],
   "source": [
    "tok_rep = TokenizationRepairer.from_pretrained(\"eo_large_arxiv_with_errors\", device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "27d26b55-4147-4d51-859e-468a33a7c5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "trt_tok = tok_rep.model.encoder.tokenizer\n",
    "trt_chars = [trt_tok.id_to_token(i) for i in range(trt_tok.get_vocab_size())]\n",
    "trt_tok.get_vocab_size(), trt_chars\n",
    "assert all(char in trt_chars for char in char_tok.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "84754210-8815-4209-a15e-34bdf4ddba14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<mask>', '<sep>'}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(trt_tok.get_vocab()) - set(char_tok.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "492a35ac-0565-493d-a473-a95d2dedf8cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.0419, -0.0796,  0.0186,  ...,  0.0085, -0.0026, -0.0574],\n",
      "        [ 0.0011, -0.0510, -0.0250,  ...,  0.0011,  0.1244, -0.0295],\n",
      "        [ 0.0341, -0.0372, -0.0035,  ..., -0.0037,  0.0338,  0.0133],\n",
      "        ...,\n",
      "        [ 0.0149, -0.0206,  0.0012,  ...,  0.0372,  0.0211, -0.0229],\n",
      "        [-0.0441, -0.0076, -0.0076,  ...,  0.0067, -0.0176, -0.0320],\n",
      "        [-0.0469,  0.0433, -0.0172,  ...,  0.0054, -0.0747,  0.0176]],\n",
      "       requires_grad=True)\n",
      "torch.Size([99, 512]) tensor([[ 0.0051,  0.0053,  0.0026,  ..., -0.0388, -0.0927,  0.0241],\n",
      "        [ 0.0167, -0.0090,  0.0196,  ..., -0.1209,  0.0299, -0.1390],\n",
      "        [ 0.0126,  0.0199,  0.0080,  ..., -0.1087, -0.0695,  0.0321],\n",
      "        ...,\n",
      "        [ 0.0006,  0.0082,  0.0017,  ..., -0.0256, -0.0366,  0.0614],\n",
      "        [ 0.0078, -0.0013, -0.0002,  ..., -0.0140,  0.0141,  0.0621],\n",
      "        [ 0.0126,  0.0072,  0.0063,  ...,  0.0174, -0.0390, -0.0065]])\n"
     ]
    }
   ],
   "source": [
    "print(nsc_model.embedding.embedding.emb.weight)\n",
    "new_embedding_weight = torch.stack([\n",
    "    tok_rep.model.encoder.embedding.embedding.weight[trt_tok.token_to_id(char_tok.id_to_token(i))]\n",
    "    for i in range(char_tok.vocab_size)\n",
    "])\n",
    "print(new_embedding_weight.shape, new_embedding_weight)\n",
    "assert all(torch.equal(new_embedding_weight[char_tok.token_to_id(char)], tok_rep.model.encoder.embedding.embedding.weight[trt_tok.token_to_id(char)]) \n",
    "           for char in char_tok.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "5ca31294-125d-40c7-94d5-d3222e445116",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nsc_model.embedding.embedding.emb.weight.data = new_embedding_weight\n",
    "nsc_model.embedding.norm.load_state_dict(tok_rep.model.encoder.embedding.norm.state_dict())\n",
    "nsc_model.encoder.encoder.load_state_dict(tok_rep.model.encoder.encoder.state_dict())\n",
    "nsc_model.head.clf.load_state_dict(tok_rep.model.head.head.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "66ceb76c-01f5-4199-803e-d693fb643a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = \"thisisatest\"\n",
    "positions = torch.arange(len(\"thisisatest\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9c15ad5b-0742-4e1a-9fd3-c818416aed08",
   "metadata": {},
   "outputs": [],
   "source": [
    "nsc_token_emb = nsc_model.embedding.embedding(torch.tensor(char_tok.tokenize(seq)))\n",
    "nsc_pos_emb = nsc_model.embedding.pos_emb(positions)\n",
    "nsc_emb = nsc_model.embedding(torch.tensor(char_tok.tokenize(seq)).unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "8f0481ed-279f-41b2-88c9-15f254470387",
   "metadata": {},
   "outputs": [],
   "source": [
    "ipt_ids = torch.tensor([trt_tok.encode(seq).ids[1:-1]])\n",
    "trt_token_emb = tok_rep.model.encoder.embedding.embedding(ipt_ids.T)\n",
    "trt_pos_emb = tok_rep.model.encoder.embedding.pos_embedding(ipt_ids.T)\n",
    "trt_emb = tok_rep.model.encoder.embedding(ipt_ids.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "92fed37e-b3cf-4f31-86b5-c24530ac22da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([11, 512]), torch.Size([11, 1, 512]))"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nsc_token_emb.shape, trt_pos_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ed104278-1d43-420f-9ba7-dd9e503cf8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "trt_all_emb = tok_rep.model.encoder.embedding.norm(trt_token_emb * 512 ** 0.5 + trt_pos_emb)\n",
    "nsc_all_emb = nsc_model.embedding.norm(nsc_token_emb + nsc_pos_emb)\n",
    "assert torch.allclose(trt_all_emb[:, 0, :], nsc_all_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "9dc78530-ebe3-4831-aa72-fc8d1ffc9c2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True, True)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "    torch.allclose(nsc_token_emb, trt_token_emb[:, 0, :] * 512 ** 0.5), \n",
    "    torch.allclose(nsc_pos_emb, trt_pos_emb[:, 0, :]), \n",
    "    torch.allclose(nsc_emb[0], trt_emb[:, 0, :])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "4d322467-b1c3-4bc2-acd9-9b10af1c19e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "nsc_enc = nsc_model.encoder(nsc_emb)\n",
    "trt_enc = tok_rep.model.encoder.encoder(trt_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "567118ac-f769-483a-8646-cb75e94a6096",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.allclose(nsc_enc[0], trt_enc[:, 0, :], atol=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ec5ba48b-aac7-42a7-a29a-68500cb05871",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(os.path.join(EXPERIMENT_DIR, \"TOKENIZATION_REPAIR\", \"eo_large_arxiv_with_errors_ported\", \"checkpoints\"), exist_ok=True)\n",
    "io.save_checkpoint(os.path.join(EXPERIMENT_DIR, \"TOKENIZATION_REPAIR\", \"eo_large_arxiv_with_errors_ported\", \"checkpoints\", \"checkpoint_best.pt\"), \n",
    "                   nsc_model, 0, 0)\n",
    "with open(os.path.join(EXPERIMENT_DIR, \"TOKENIZATION_REPAIR\", \"eo_large_arxiv_with_errors_ported\", \"cfg.pkl\"), \"wb\") as pf:\n",
    "    pickle.dump((dummy_train_cfg, {}), pf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab81376-8706-4bbd-82e5-40d0439201ca",
   "metadata": {
    "tags": []
   },
   "source": [
    "### EO small (3 layer transformer encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "68271d56-f442-466b-a8fb-24aa13fa6b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_cfg = embedding.TensorEmbeddingConfig()\n",
    "model_cfg = ModelForTokenClassificationConfig(\n",
    "    type=Models.MODEL_FOR_TOKEN_CLASSIFICATION, \n",
    "    hidden_dim=512,\n",
    "    num_classes=3,\n",
    "    tokenizer=char_tok_cfg,\n",
    "    embedding=embedding_cfg,\n",
    "    num_layers=3,\n",
    "    activation=\"gelu\",\n",
    "    feed_forward_dim=2048,\n",
    "    norm=False,\n",
    "    num_clf_layers=1\n",
    ")\n",
    "sample_inputs = Batch([torch.tensor([0, 1])], {})\n",
    "nsc_model = ModelForTokenClassification(sample_inputs=sample_inputs, cfg=model_cfg, device=\"cpu\").eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "1309e891-38b6-4f6e-a75d-a523aad9ed7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': {'type': <Models.MODEL_FOR_TOKEN_CLASSIFICATION: 5>, 'max_length': 512, 'hidden_dim': 512, 'tokenizer': {'type': <Tokenizers.CHAR: 1>, 'file_path': None}, 'embedding': {'learned_position_embedding': False, 'embed_positions': True, 'dropout': 0.1}, 'dropout': 0.1, 'num_layers': 3, 'feed_forward_dim': 2048, 'norm': False, 'activation': 'gelu', 'num_clf_layers': 1, 'num_classes': 3}, 'variant': {'type': <DatasetVariants.TOKENIZATION_REPAIR: 3>, 'data_scheme': 'tensor', 'input_type': 'char', 'add_bos_eos': True}, 'experiment_dir': '???', 'experiment_name': 'eo_small_arxiv_with_errors', 'data_dir': '???', 'datasets': '???', 'dataset_limits': '???', 'val_splits': '???', 'epochs': 20, 'batch_size': 64, 'batch_max_length': None, 'bucket_span': None, 'optimizer': '???', 'lr_scheduler': None, 'log_per_epoch': 100, 'eval_per_epoch': 4, 'keep_last_n_checkpoints': 0, 'seed': 22, 'num_workers': None, 'pin_memory': True, 'mixed_precision': True, 'start_from_checkpoint': None, 'exponential_moving_average': None}\n"
     ]
    }
   ],
   "source": [
    "variant_cfg = variants.TokenizationRepairConfig(\n",
    "    type=variants.DatasetVariants.TOKENIZATION_REPAIR,\n",
    "    data_scheme=\"tensor\",\n",
    "    input_type=\"char\",\n",
    "    add_bos_eos=True\n",
    ")\n",
    "dummy_train_cfg = OmegaConf.structured(\n",
    "    config.TrainConfig(\n",
    "        experiment_name=\"eo_small_arxiv_with_errors\",\n",
    "        variant=variant_cfg,\n",
    "        model=model_cfg\n",
    "    )\n",
    ")\n",
    "print(dummy_train_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "f24a8891-0cdf-4a39-8330-e4d830348c2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-10 22:06:45,270 [DOWNLOAD] [INFO] model eo_small_arxiv_with_errors was already downloaded to cache directory /home/sebastian/anaconda3/envs/masters_thesis/lib/python3.8/site-packages/trt/api/.cache\n",
      "2022-05-10 22:06:45,292 [TOKENIZATION_REPAIR] [INFO] running tokenization repair on device Intel(R) Core(TM) i7-9750H CPU @ 2.60GHz\n"
     ]
    }
   ],
   "source": [
    "tok_rep = TokenizationRepairer.from_pretrained(\"eo_small_arxiv_with_errors\", device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "bffc81ac-6e58-4555-8e2f-a853a09be969",
   "metadata": {},
   "outputs": [],
   "source": [
    "trt_tok = tok_rep.model.encoder.tokenizer\n",
    "trt_chars = [trt_tok.id_to_token(i) for i in range(trt_tok.get_vocab_size())]\n",
    "trt_tok.get_vocab_size(), trt_chars\n",
    "assert all(char in trt_chars for char in char_tok.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "5e7a8a1f-f590-494c-bbd8-7747a98654ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<mask>', '<sep>'}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(trt_tok.get_vocab()) - set(char_tok.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "839f1f86-f422-451b-bc8a-043001ed8c01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.0407, -0.0709, -0.0296,  ...,  0.0186, -0.0709,  0.0200],\n",
      "        [ 0.0227,  0.0843,  0.0452,  ...,  0.0493, -0.0047,  0.0124],\n",
      "        [ 0.0359, -0.0234, -0.0283,  ...,  0.0187,  0.0263, -0.1021],\n",
      "        ...,\n",
      "        [-0.0265, -0.0368,  0.0386,  ...,  0.0050,  0.0590, -0.0624],\n",
      "        [ 0.0355, -0.0283,  0.0601,  ..., -0.0126, -0.1097, -0.0459],\n",
      "        [-0.0817, -0.0882,  0.0697,  ...,  0.0685,  0.0540, -0.0507]],\n",
      "       requires_grad=True)\n",
      "torch.Size([99, 512]) tensor([[ 0.0108,  0.0098,  0.0099,  ..., -0.0142, -0.1065, -0.0008],\n",
      "        [ 0.0157, -0.0226,  0.0119,  ..., -0.0181, -0.0790, -0.0182],\n",
      "        [ 0.0054,  0.0153,  0.0057,  ..., -0.0999,  0.0133, -0.0816],\n",
      "        ...,\n",
      "        [ 0.0104,  0.0077,  0.0080,  ...,  0.0667, -0.0535,  0.0512],\n",
      "        [ 0.0111,  0.0100,  0.0081,  ..., -0.1114,  0.0735, -0.0356],\n",
      "        [ 0.0124,  0.0126,  0.0116,  ..., -0.0277, -0.0135, -0.0464]])\n"
     ]
    }
   ],
   "source": [
    "print(nsc_model.embedding.embedding.emb.weight)\n",
    "new_embedding_weight = torch.stack([\n",
    "    tok_rep.model.encoder.embedding.embedding.weight[trt_tok.token_to_id(char_tok.id_to_token(i))]\n",
    "    for i in range(char_tok.vocab_size)\n",
    "])\n",
    "print(new_embedding_weight.shape, new_embedding_weight)\n",
    "assert all(torch.equal(new_embedding_weight[char_tok.token_to_id(char)], tok_rep.model.encoder.embedding.embedding.weight[trt_tok.token_to_id(char)]) \n",
    "           for char in char_tok.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "4b28589c-561d-42bc-905f-a654cd5ef4e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nsc_model.embedding.embedding.emb.weight.data = new_embedding_weight\n",
    "nsc_model.embedding.norm.load_state_dict(tok_rep.model.encoder.embedding.norm.state_dict())\n",
    "nsc_model.encoder.encoder.load_state_dict(tok_rep.model.encoder.encoder.state_dict())\n",
    "nsc_model.head.clf.load_state_dict(tok_rep.model.head.head.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "4885a539-8b0b-43b3-8e43-e11f9e239c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = \"thisisatest\"\n",
    "positions = torch.arange(len(\"thisisatest\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "d7be486b-1805-4a69-be40-265381243143",
   "metadata": {},
   "outputs": [],
   "source": [
    "nsc_token_emb = nsc_model.embedding.embedding(torch.tensor(char_tok.tokenize(seq)))\n",
    "nsc_pos_emb = nsc_model.embedding.pos_emb(positions)\n",
    "nsc_emb = nsc_model.embedding(torch.tensor(char_tok.tokenize(seq)).unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "43a84f93-4f7c-46a9-b790-06eae1857364",
   "metadata": {},
   "outputs": [],
   "source": [
    "ipt_ids = torch.tensor([trt_tok.encode(seq).ids[1:-1]])\n",
    "trt_token_emb = tok_rep.model.encoder.embedding.embedding(ipt_ids.T)\n",
    "trt_pos_emb = tok_rep.model.encoder.embedding.pos_embedding(ipt_ids.T)\n",
    "trt_emb = tok_rep.model.encoder.embedding(ipt_ids.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "e62fbef0-5503-4260-a754-7388efd5b54b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([11, 512]), torch.Size([11, 1, 512]))"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nsc_token_emb.shape, trt_pos_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "5e77e8df-6747-47bc-b088-a8b4effdfb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trt_all_emb = tok_rep.model.encoder.embedding.norm(trt_token_emb * 512 ** 0.5 + trt_pos_emb)\n",
    "nsc_all_emb = nsc_model.embedding.norm(nsc_token_emb + nsc_pos_emb)\n",
    "assert torch.allclose(trt_all_emb[:, 0, :], nsc_all_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "25b144a3-65a7-4dd4-87be-19a684908b8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True, True)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "    torch.allclose(nsc_token_emb, trt_token_emb[:, 0, :] * 512 ** 0.5), \n",
    "    torch.allclose(nsc_pos_emb, trt_pos_emb[:, 0, :]), \n",
    "    torch.allclose(nsc_emb[0], trt_emb[:, 0, :])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "a144f66f-8043-4082-81a0-22a9a5455838",
   "metadata": {},
   "outputs": [],
   "source": [
    "nsc_enc = nsc_model.encoder(nsc_emb)\n",
    "trt_enc = tok_rep.model.encoder.encoder(trt_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "3304cac2-f7a3-42ac-91db-336cd3d9d26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.allclose(nsc_enc[0], trt_enc[:, 0, :], atol=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "8a8c7cd3-e47d-446d-8180-5f30f3d88af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(os.path.join(EXPERIMENT_DIR, \"TOKENIZATION_REPAIR\", \"eo_small_arxiv_with_errors_ported\", \"checkpoints\"), exist_ok=True)\n",
    "io.save_checkpoint(os.path.join(EXPERIMENT_DIR, \"TOKENIZATION_REPAIR\", \"eo_small_arxiv_with_errors_ported\", \"checkpoints\", \"checkpoint_best.pt\"), \n",
    "                   nsc_model, 0, 0)\n",
    "with open(os.path.join(EXPERIMENT_DIR, \"TOKENIZATION_REPAIR\", \"eo_small_arxiv_with_errors_ported\", \"cfg.pkl\"), \"wb\") as pf:\n",
    "    pickle.dump((dummy_train_cfg, {}), pf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17f6cf8-a8e2-4f1f-8af0-56eaef6e1953",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
