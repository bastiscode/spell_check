#!/usr/bin/env python

import argparse
import logging
import os
import sys
import time
from typing import Tuple, Optional, List, Dict

import torch
from torch.backends import cudnn

import nsc
from nsc import SpellingErrorCorrector, get_available_spelling_error_correction_models, GreedySearch, SampleSearch, \
    BeamSearch, BestFirstSearch, Score
from nsc.api.sec import Search
from nsc.api.utils import load_text_file, generate_report, save_text_file
from nsc.modules import inference
from nsc.utils import common, edit
from nsc.tasks import utils as task_utils


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(
        "Spelling error correction using Transformers and Graph Neural Networks",
        description="Correct spelling errors using Transformer and Graph Neural Network models."
    )
    parser.add_argument(
        "-m",
        "--model",
        choices=[model.name for model in get_available_spelling_error_correction_models()],
        default=get_available_spelling_error_correction_models()[0].name,
        help="Name of the model to use for spelling error correction"
    )
    parser.add_argument(
        "--search",
        choices=["greedy", "sample", "beam", "best_first"],
        default="greedy",
        help="Set the search method to be used during decoding"
    )
    parser.add_argument(
        "--sample-top-k",
        type=int,
        default=5,
        help="Sample one from the best k candidates"
    )
    parser.add_argument(
        "--beam-width",
        type=int,
        default=5,
        help="Always keep the best beam-width candidates"
    )
    parser.add_argument(
        "--score",
        choices=["log_likelihood", "dictionary", "diff_from_input"],
        default="log_likelihood",
        help="Set the scoring method to score potential solution candidates during decoding"
    )
    parser.add_argument(
        "--prefix-index",
        type=str,
        default=None,
        help="Path to a prefix index that used when score is not log_likelihood, if not specified a default prefix "
             "index is provided"
    )
    input_group = parser.add_mutually_exclusive_group()
    input_group.add_argument(
        "-c",
        "--correct",
        type=str,
        default=None,
        help="Correct spelling errors in some text"
    )
    input_group.add_argument(
        "-f",
        "--file",
        type=str,
        default=None,
        help="Path to a text file which will be corrected line by line"
    )
    input_group.add_argument(
        "-i",
        "--interactive",
        action="store_true",
        default=None,
        help="Start an interactive session where your command line input is corrected"
    )
    parser.add_argument(
        "-df",
        "--detection-file",
        type=str,
        default=None,
        help="Path to a text file which contains outputs of a detection model"
    )
    parser.add_argument(
        "-o",
        "--out-path",
        type=str,
        default=None,
        help="Path where corrected file should be saved to"
    )
    parser.add_argument(
        "--cpu",
        action="store_true",
        help="Force to run the model on CPU, by default a GPU is used if available"
    )
    parser.add_argument(
        "-b",
        "--batch-size",
        type=int,
        default=16,
        help="Determines how many inputs will be processed at the same time, larger values should usually result in "
             "faster repairing but require more memory"
    )
    parser.add_argument(
        "--batch-max-length-factor",
        type=float,
        default=None,
        help="Process as many inputs at the same as fit within a maximum number of tokens calculated as "
             "batch_max_length_factor * model_max_length. The model_max_length is the maximum input length in tokens a "
             "model can handle. It is a fixed value defined by the model. "
             "Larger factors should usually result in faster repairing but require more memory. "
             "If this is specified it will take precedence over batch-size."
    )
    parser.add_argument(
        "-u",
        "--unsorted",
        action="store_true",
        help="Disable sorting of the inputs before processing (for a large number of inputs or large text files "
             "sorting the sequences beforehand leads to speed ups because it minimizes the amount of padding needed "
             "within a batch of sequences)"
    )
    parser.add_argument(
        "--suggest",
        type=int,
        default=None,
        help="Output this many suggestions for corrections, can be used together with --correct or --interactive"
    )
    parser.add_argument(
        "--sed-in",
        action="store_true",
        help="Whether the input also contains spelling error detections (should be used together with nsed --sec-out)"
    )
    parser.add_argument(
        "-e",
        "--experiment",
        type=str,
        default=None,
        help="Path to an experiment directory from which the model will be loaded "
             "(use this when you trained your own model and want to use it)"
    )
    parser.add_argument(
        "-l",
        "--list",
        action="store_true",
        help="List all available models with short descriptions"
    )
    parser.add_argument(
        "-p",
        "--pipe",
        action="store_true",
        help="Pass this flag when using nsec in a pipe because input and output is then treated as an iterator "
             "(note that sorting by length gets disabled with this flag because it is not possible to sort an "
             "iterator)"
    )
    parser.add_argument(
        "--precision",
        choices=["fp32", "fp16", "bfp16"],
        default="fp32",
        help="Choose the precision for inference, fp16 or bfp16 can result in faster runtimes when running on a "
             "new GPU that supports lower precision, but it can be slower on older GPUs."
    )
    parser.add_argument(
        "--progress",
        action="store_true",
        help="Show a progress bar (this flag is only respected when getting input from stdin, "
             "in interactive mode with -i progress is never shown, "
             "when repairing a file with -f progress is always shown)"
    )
    parser.add_argument(
        "-v",
        "--version",
        action="store_true",
        help="Print the version of the spell checking library"
    )
    parser.add_argument(
        "--force-download",
        action="store_true",
        help="Download the model, data and configs again even if they were already downloaded"
    )
    parser.add_argument(
        "--report",
        type=str,
        default=None,
        help="Save a runtime report (ignoring startup time) formatted as markdown table to a file, append new line "
             "if file already exists (use only when running on files or on stdin without --pipe)"
    )
    parser.add_argument(
        "--tr-plus-no-repair",
        action="store_true",
        help="Disable repairing whitespaces before correcting when using a tokenization repair plus model"
    )
    parser.add_argument(
        "--tr-plus-no-detect",
        action="store_true",
        help="Disable detecting spelling errors before correcting when using a tokenization repair plus model"
    )
    parser.add_argument(
        "--tr-plus-threshold",
        type=float,
        default=0.5,
        help="Set the spelling error detection threshold when using a tokenization repair plus model"
    )
    parser.add_argument(
        "--convert-output",
        choices=["word_detections", "sequence_detections"],
        default=None,
        help="Convert the output of spelling error correction to word level or sequence level spelling error "
             "detections."
    )
    return parser.parse_args()


def process_input(line: str, sed_in: bool) -> Tuple[str, Optional[List[int]]]:
    line = line.strip()
    if sed_in:
        split = line.rsplit("\t", 1)
        assert len(split) == 2, "expected each input line to be of format <input>\\t<detection> when " \
                                f"sed_in is specified, but got \"{line}\""
        line, detection = split
        return line.strip(), [int(det) for det in detection.split()]
    else:
        return line, None


def format_output(correction: str, sequence: str, convert_output: Optional[str]) -> str:
    return format_output_batch([correction], [sequence], convert_output)[0]


def format_output_batch(corrections: List[str], sequences: List[str], convert_outputs: Optional[str]) -> List[str]:
    if convert_outputs is None:
        return corrections
    elif convert_outputs == "word_detections":
        outputs = []
        edited_in_sequences, _ = edit.get_edited_words(sequences, corrections)
        for sequence, edited in zip(sequences, edited_in_sequences):
            outputs.append(inference.inference_output_to_str(
                [int(i in edited) for i in range(len(sequence.split()))]
            ))
        return outputs
    elif convert_outputs == "sequence_detections":
        return [inference.inference_output_to_str(int(correction != sequence))
                for correction, sequence in zip(corrections, sequences)]
    else:
        raise RuntimeError("should not happen")


def correct_and_print(
        corrector: SpellingErrorCorrector,
        text: str,
        sed_in: bool,
        suggest: Optional[int],
        search: Search,
        score: Score,
        convert_output: Optional[str],
        **corrector_kwargs: Dict
) -> None:
    line, detection = process_input(text, sed_in)
    if suggest is None:
        print(format_output(
            corrector.correct_text(
                line, detection, search=search, score=score, sort_by_length=False, **corrector_kwargs
            ),
            line,
            convert_output
        ))
    else:
        for suggestion in corrector.suggest(line, suggest, detection, **corrector_kwargs):
            print(format_output(suggestion, line, convert_output))


def run(args: argparse.Namespace) -> None:
    torch.backends.cudnn.benchmark = True
    torch.set_num_threads(len(os.sched_getaffinity(0)))
    torch.use_deterministic_algorithms(False)

    if args.version:
        print(f"nsec version {nsc.__version__}")
        return
    if args.list:
        model_str = "\n".join(
            f"- [task: {model.task}] {model.name}: {model.description}"
            for model in get_available_spelling_error_correction_models()
        )
        print(f"Available models:\n{model_str}")
        return

    if args.search == "greedy":
        search = GreedySearch()
    elif args.search == "sample":
        search = SampleSearch(top_k=args.sample_top_k)
    elif args.search == "beam":
        search = BeamSearch(beam_width=args.beam_width)
    elif args.search == "best_first":
        # best first search is only implemented unbatched, set batch size alyways to 1 such that progress is reported
        # more frequently if enabled
        args.batch_size = 1
        search = BestFirstSearch()
    else:
        raise RuntimeError("should not happen")

    if args.score != "log_likelihood":
        assert args.search == "best_first", \
            "to use a score other than log_likelihood you need to set the search method to " \
            "best_first search, because its the only search method supporting different scoring modes for now"
    score = Score(mode=args.score, prefix_index=args.prefix_index)

    if args.experiment:
        corrector = SpellingErrorCorrector.from_experiment(
            experiment_dir=args.experiment,
            device="cpu" if args.cpu else "cuda"
        )
    else:
        corrector = SpellingErrorCorrector.from_pretrained(
            model=args.model,
            device="cpu" if args.cpu else "cuda",
            force_download=args.force_download
        )

    corrector.set_precision(args.precision)
    corrector_kwargs = {
        "tokenization_repair_plus_no_repair": args.tr_plus_no_repair,
        "tokenization_repair_plus_no_detect": args.tr_plus_no_detect,
        "tokenization_repair_plus_threshold": args.tr_plus_threshold,
    }

    start = time.perf_counter()
    num_sequences = -1
    num_bytes = -1
    if args.correct is not None:
        correct_and_print(
            corrector,
            args.correct,
            args.sed_in,
            args.suggest,
            search,
            score,
            args.convert_output,
            **corrector_kwargs
        )

    elif args.file is not None:
        lines = load_text_file(args.file)

        num_bytes = 0
        inputs = []
        for line in lines:
            line, detection = process_input(line, False)
            inputs.append(line)
            num_bytes += len(line.encode("utf8"))

        num_sequences = len(inputs)

        if args.detection_file:
            detections = load_text_file(args.detection_file)
            detections = [[int(det) for det in detection.split()] for detection in detections]
        else:
            detections = None

        corrections = corrector.correct_text(
            inputs=inputs,
            detections=detections,
            search=search,
            score=score,
            batch_size=args.batch_size,
            batch_max_length_factor=args.batch_max_length_factor,
            sort_by_length=not args.unsorted,
            show_progress=True,
            **corrector_kwargs
        )

        corrections = format_output_batch(corrections, inputs, args.convert_output)

        if args.out_path is None:
            for correction in corrections:
                print(correction)
        else:
            save_text_file(args.out_path, corrections)

    elif args.interactive:
        while True:
            try:
                line = input()
                correct_and_print(
                    corrector,
                    line,
                    args.sed_in,
                    args.suggest,
                    search,
                    score,
                    args.convert_output,
                    **corrector_kwargs
                )
            except KeyboardInterrupt:
                return
    else:
        if sys.stdin.isatty():
            return

        try:
            if args.pipe:
                for line in sys.stdin:
                    line, detection = process_input(line, args.sed_in)
                    print(format_output(
                        corrector.correct_text(
                            inputs=line,
                            detections=detection,
                            search=search,
                            score=score,
                            sort_by_length=False,
                            **corrector_kwargs
                        ),
                        line,
                        args.convert_output
                    ))
            else:
                inputs = []
                detections = []
                num_bytes = 0
                for line in sys.stdin:
                    line, detection = process_input(line, args.sed_in)
                    inputs.append(line)
                    detections.append(detection)
                    num_bytes += len(line.encode("utf8"))

                num_sequences = len(inputs)

                for corrected_line in format_output_batch(
                        corrector.correct_text(
                            inputs=inputs,
                            detections=detections if args.sed_in else None,
                            search=search,
                            score=score,
                            batch_size=args.batch_size,
                            batch_max_length_factor=args.batch_max_length_factor,
                            sort_by_length=not args.unsorted,
                            show_progress=args.progress,
                            **corrector_kwargs
                        ),
                        inputs,
                        args.convert_output
                ):
                    print(corrected_line)

        except BrokenPipeError:
            return
        except Exception as e:
            raise e

    if args.report and num_sequences >= 0 and num_bytes >= 0:
        end = time.perf_counter()

        # for reporting model parameters create new model like before training, so we can record
        # fixed parameters, unused parameters and trainable parameters
        sample_inputs = corrector.task.generate_sample_inputs(2)
        cpu = torch.device("cpu")
        report_model = corrector.task.get_model(sample_inputs, corrector.model.cfg, cpu)
        inputs, _ = corrector.task._prepare_inputs_and_labels(sample_inputs, cpu)
        unused_parameters = task_utils.get_unused_parameters(report_model, **inputs)
        task_utils.disable_parameters(report_model, unused_parameters)
        parameter_dict = common.get_num_parameters(report_model, unused_parameters)

        generate_report(
            corrector.task_name,
            corrector.model_name,
            parameter_dict,
            num_sequences,
            num_bytes,
            end - start,
            corrector._mixed_precision_dtype,
            args.batch_size,
            not args.unsorted,
            corrector.device,
            file_path=args.report
        )


if __name__ == "__main__":
    # disable logging since we do not want that for our command line interface
    logging.disable(logging.CRITICAL)
    run(parse_args())
