#!/usr/bin/env python

import argparse
import logging
import os
import sys
import time
from typing import List

import torch
from torch.backends import cudnn

import nsc
from nsc import SpellingErrorDetector, get_available_spelling_error_detection_models
from nsc.api.utils import load_text_file, generate_report, save_text_file
from nsc.modules import inference
from nsc.tasks import utils as task_utils
from nsc.utils import common


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(
        "Spelling error detection using Transformers and Graph Neural Networks",
        description="Detect spelling errors (on sequence or word level) using Transformer or "
                    "Graph Neural Network models."
    )
    default_model = get_available_spelling_error_detection_models()[0]
    parser.add_argument(
        "-m",
        "--model",
        choices=[f"{model.task}:{model.name}" for model in get_available_spelling_error_detection_models()],
        default=f"{default_model.task}:{default_model.name}",
        help="Name of the model to use for spelling error detection"
    )
    input_group = parser.add_mutually_exclusive_group()
    input_group.add_argument(
        "-d",
        "--detect",
        type=str,
        default=None,
        help="Detect spelling errors in some text"
    )
    input_group.add_argument(
        "-f",
        "--file",
        type=str,
        default=None,
        help="Path to a text file which will be checked for spelling errors line by line"
    )
    input_group.add_argument(
        "-i",
        "--interactive",
        action="store_true",
        default=None,
        help="Start an interactive session where your command line input is checked for spelling errors"
    )
    parser.add_argument(
        "-o",
        "--out-path",
        type=str,
        default=None,
        help="Path where spelling error detections should be saved to"
    )
    parser.add_argument(
        "--cpu",
        action="store_true",
        help="Force to run the model on CPU, by default a GPU is used if available"
    )
    parser.add_argument(
        "-b",
        "--batch-size",
        type=int,
        default=16,
        help="Determines how many inputs will be processed at the same time, larger values should usually result in "
             "faster repairing but require more memory"
    )
    parser.add_argument(
        "-u",
        "--unsorted",
        action="store_true",
        help="Disable sorting of the inputs before processing (for a large number of inputs or large text files "
             "sorting the sequences beforehand leads to speed ups because it minimizes the amount of padding needed "
             "within a batch of sequences)"
    )
    parser.add_argument(
        "-t",
        "--threshold",
        type=float,
        default=0.5,
        help="Minimum probability of a prediction to be considered as a spelling error detection"
    )
    parser.add_argument(
        "--sec-out",
        action="store_true",
        help="Whether to format the output suitable for running spelling error correction on it, outputs both "
             "the original inputs and the spelling error detections (should be used together with nsec --sed-in)"
    )
    parser.add_argument(
        "-e",
        "--experiment",
        type=str,
        default=None,
        help="Path to an experiment directory from which the model will be loaded "
             "(use this when you trained your own model and want to use it)"
    )
    parser.add_argument(
        "-l",
        "--list",
        action="store_true",
        help="List all available models with short descriptions"
    )
    parser.add_argument(
        "-p",
        "--pipe",
        action="store_true",
        help="Pass this flag when using nsec in a pipe because input and output is then treated as an iterator "
             "(note that sorting by length gets disabled with this flag because it is not possible to sort an "
             "iterator)"
    )
    parser.add_argument(
        "--precision",
        choices=["fp32", "fp16", "bfp16"],
        default="fp32",
        help="Choose the precision for inference, fp16 or bfp16 can result in faster runtimes when running on a "
             "new GPU that supports lower precision, but it can be slower on older GPUs."
    )
    parser.add_argument(
        "--progress",
        action="store_true",
        help="Show a progress bar (this flag is only respected when getting input from stdin, "
             "in interactive mode with -i progress is never shown, "
             "when repairing a file with -f progress is always shown)"
    )
    parser.add_argument(
        "-v",
        "--version",
        action="store_true",
        help="Print the version of the spell checking library"
    )
    parser.add_argument(
        "--force-download",
        action="store_true",
        help="Download the model, data and configs again even if they were already downloaded"
    )
    parser.add_argument(
        "--report",
        type=str,
        default=None,
        help="Save a runtime report (ignoring startup time) formatted as markdown table to a file, append new line "
             "if file already exists"
    )
    return parser.parse_args()


def format_output(detection: List[int], org_input: str, sec_out: bool) -> str:
    detection_str = inference.inference_output_to_str(detection)
    if sec_out:
        return f"{org_input}\t{detection_str}"
    else:
        return detection_str


def run(args: argparse.Namespace) -> None:
    torch.backends.cudnn.benchmark = True
    torch.set_num_threads(len(os.sched_getaffinity(0)))
    torch.use_deterministic_algorithms(False)

    if args.version:
        print(f"nsed version {nsc.__version__}")
        return
    if args.list:
        model_str = "\n".join(
            f"- [task: {model.task}] {model.name}: {model.description}"
            for model in get_available_spelling_error_detection_models()
        )
        print(f"Available models:\n{model_str}")
        return

    if args.experiment:
        detector = SpellingErrorDetector.from_experiment(
            experiment_dir=args.experiment,
            device="cpu" if args.cpu else "cuda"
        )
    else:
        task, model = args.model.split(":")
        detector = SpellingErrorDetector.from_pretrained(
            task=task,
            model=model,
            device="cpu" if args.cpu else "cuda",
            force_download=args.force_download
        )

    detector.set_precision(args.precision)

    start = time.perf_counter()
    num_sequences = -1
    num_characters = -1
    if args.detect is not None:
        detection = detector.detect_text(args.detect, args.threshold, sort_by_length=False)
        print(format_output(detection, args.detect, args.sec_out))

    elif args.file is not None:
        inputs = load_text_file(args.file)
        num_sequences = len(inputs)
        num_characters = sum(len(ipt) for ipt in inputs)

        detections = detector.detect_text(
            inputs=inputs,
            threshold=args.threshold,
            batch_size=args.batch_size,
            sort_by_length=not args.unsorted,
            show_progress=True
        )
        if args.out_path is None:
            for detection, line in zip(detections, inputs):
                print(format_output(detection, line, args.sec_out))
        else:
            save_text_file(
                args.out_path,
                [format_output(detection, line, args.sec_out) for detection, line in zip(detections, inputs)]
            )

    elif args.interactive:
        while True:
            try:
                line = input()
                detection = detector.detect_text(line, args.threshold)
                print(format_output(detection, line, args.sec_out))
            except KeyboardInterrupt:
                return

    else:
        if sys.stdin.isatty():
            return

        try:
            if args.pipe:
                for line in sys.stdin:
                    line = line.strip()
                    detection = detector.detect_text(line, args.threshold, sort_by_length=False)
                    print(format_output(detection, line, args.sec_out))
            else:
                lines = [line.strip() for line in sys.stdin]
                detections = detector.detect_text(
                    inputs=lines,
                    threshold=args.threshold,
                    batch_size=args.batch_size,
                    sort_by_length=not args.unsorted,
                    show_progress=args.progress
                )
                for detection, line in zip(detections, lines):
                    print(format_output(detection, line, args.sec_out))

                num_sequences = len(lines)
                num_characters = sum(len(line) for line in lines)

        except BrokenPipeError:
            return
        except Exception as e:
            raise e

    if args.report and num_sequences >= 0 and num_characters >= 0:
        end = time.perf_counter()

        sample_inputs = detector.task.generate_sample_inputs(2)
        inputs, _ = detector.task._prepare_inputs_and_labels(sample_inputs, detector.device)
        unused_parameters = task_utils.get_unused_parameters(detector.model, **inputs)
        parameter_dict = common.get_num_parameters(detector.model, unused_parameters)

        generate_report(
            detector.task_name,
            detector.model_name,
            parameter_dict["total"] - parameter_dict["unused"],
            num_sequences,
            num_characters,
            end - start,
            detector._mixed_precision_dtype,
            args.batch_size,
            not args.unsorted,
            detector.device,
            file_path=args.report
        )


if __name__ == "__main__":
    # disable logging since we do not want that for our command line interface
    logging.disable(logging.CRITICAL)
    run(parse_args())
