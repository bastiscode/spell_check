#!/usr/bin/env python

import argparse
import logging
import os
import sys
import time

import torch
import torch.backends.cudnn

import nsc
from nsc import get_available_tokenization_repair_models, TokenizationRepairer
from nsc.api.utils import generate_report, load_text_file, save_text_file
from nsc.utils import common
from nsc.tasks import utils as task_utils


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(
        "Tokenization repair using Transformers",
        description="Repair tokenization in text by inserting missing or deleting superfluous whitespaces"
    )
    default_model = get_available_tokenization_repair_models()[0]
    parser.add_argument(
        "-m",
        "--model",
        choices=[model.name for model in get_available_tokenization_repair_models()],
        default=default_model.name,
        help="Name of the model to use for tokenization repair"
    )
    input_group = parser.add_mutually_exclusive_group()
    input_group.add_argument(
        "-r",
        "--repair",
        type=str,
        default=None,
        help="Text to repair"
    )
    input_group.add_argument(
        "-f",
        "--file",
        type=str,
        default=None,
        help="Path to a text file which will be repaired line by line"
    )
    input_group.add_argument(
        "-i",
        "--interactive",
        action="store_true",
        default=None,
        help="Start an interactive session where your command line input is repaired"
    )
    parser.add_argument(
        "-o",
        "--out-path",
        type=str,
        default=None,
        help="Path where repaired text should be saved to"
    )
    parser.add_argument(
        "--cpu",
        action="store_true",
        help="Force to run the model on CPU, by default a GPU is used if available"
    )
    parser.add_argument(
        "-b",
        "--batch-size",
        type=int,
        default=16,
        help="Determines how many inputs will be repaired at the same time, larger values should usually result in "
             "faster repairing but require more memory"
    )
    parser.add_argument(
        "--batch-max-length-factor",
        type=float,
        default=None,
        help="Process as many inputs at the same as fit within a maximum number of tokens calculated as "
             "batch_max_length_factor * model_max_length. The model_max_length is the maximum input length in tokens a "
             "model can handle. It is a fixed value defined by the model. "
             "Larger factors should usually result in faster repairing but require more memory. "
             "If this is specified it will take precedence over batch-size."
    )
    parser.add_argument(
        "-u",
        "--unsorted",
        action="store_true",
        help="Disable sorting of the inputs before repairing (for a large number of inputs or large text files sorting "
             "the sequences beforehand leads to speed ups because it minimizes the amount of padding needed "
             "within a batch of sequences)"
    )
    parser.add_argument(
        "-e",
        "--experiment",
        type=str,
        default=None,
        help="Path to an experiment directory from which the model will be loaded "
             "(use this when you trained your own model and want to use it)"
    )
    parser.add_argument(
        "-l",
        "--list",
        action="store_true",
        help="List all available models with short descriptions"
    )
    parser.add_argument(
        "-p",
        "--pipe",
        action="store_true",
        help="Pass this flag when using ntr in a pipe because input and output is then treated as an iterator "
             "(note that sorting by length gets disabled with this flag because it is not possible to sort an "
             "iterator)"
    )
    parser.add_argument(
        "--precision",
        choices=["fp32", "fp16", "bfp16"],
        default="fp32",
        help="Choose the precision for inference, fp16 or bfp16 can result in faster runtimes when running on a "
             "new GPU that supports lower precision, but it can be slower on older GPUs."
    )
    parser.add_argument(
        "--progress",
        action="store_true",
        help="Show a progress bar (this flag is only respected when getting input from stdin, "
             "in interactive mode with -i progress is never shown, "
             "when repairing a file with -f progress is always shown)"
    )
    parser.add_argument(
        "-v",
        "--version",
        action="store_true",
        help="Print the version of the spell checking library"
    )
    parser.add_argument(
        "--force-download",
        action="store_true",
        help="Download the model again even if it already was downloaded"
    )
    parser.add_argument(
        "--report",
        type=str,
        default=None,
        help="Save a runtime report (ignoring startup time) formatted as markdown table to a file, append new line "
             "if file already exists (use only when running on files or on stdin without --pipe)"
    )
    return parser.parse_args()


def run(args: argparse.Namespace) -> None:
    torch.backends.cudnn.benchmark = True
    torch.set_num_threads(len(os.sched_getaffinity(0)))
    torch.use_deterministic_algorithms(False)

    if args.version:
        print(f"ntr version {nsc.__version__}")
        return
    if args.list:
        model_str = "\n".join(
            f"- [task: {model.task}] {model.name}: {model.description}"
            for model in get_available_tokenization_repair_models()
        )
        print(f"Available models:\n{model_str}")
        return

    if args.experiment:
        tok_rep = TokenizationRepairer.from_experiment(
            experiment_dir=args.experiment,
            device="cpu" if args.cpu else "cuda"
        )
    else:
        tok_rep = TokenizationRepairer.from_pretrained(
            model=args.model,
            device="cpu" if args.cpu else "cuda",
            force_download=args.force_download
        )

    tok_rep.set_precision(args.precision)

    start = time.perf_counter()
    num_sequences = -1
    num_bytes = -1
    if args.repair is not None:
        print(tok_rep.repair_text(args.repair))

    elif args.file is not None:
        inputs = load_text_file(args.file)
        num_sequences = len(inputs)
        num_bytes = sum(len(ipt.encode("utf8")) for ipt in inputs)

        repaired_lines = tok_rep.repair_text(
            inputs=inputs,
            batch_size=args.batch_size,
            batch_max_length_factor=args.batch_max_length_factor,
            sort_by_length=not args.unsorted,
            show_progress=True
        )
        if args.out_path is None:
            for line in repaired_lines:
                print(line)
        else:
            save_text_file(args.out_path, repaired_lines)

    elif args.interactive:
        while True:
            try:
                line = input()
                print(tok_rep.repair_text(line))
            except KeyboardInterrupt:
                return
    else:
        if sys.stdin.isatty():
            return

        try:
            if args.pipe:
                # repair lines from stdin as they come
                for line in sys.stdin:
                    print(
                        tok_rep.repair_text(
                            inputs=line.strip(),
                            sort_by_length=False
                        )
                    )
            else:
                # read stdin completely, then potentially sort and repair
                lines = [line.strip() for line in sys.stdin]
                for repaired_line in tok_rep.repair_text(
                        inputs=lines,
                        batch_size=args.batch_size,
                        batch_max_length_factor=args.batch_max_length_factor,
                        sort_by_length=not args.unsorted,
                        show_progress=args.progress
                ):
                    print(repaired_line)

                num_sequences = len(lines)
                num_bytes = sum(len(line.encode("utf8")) for line in lines)

        except BrokenPipeError:
            return
        except Exception as e:
            raise e

    if args.report and num_sequences >= 0 and num_bytes >= 0:
        end = time.perf_counter()

        # temporarily enable autograd to compute potentially unused parameters
        tok_rep.model.requires_grad_(True)
        sample_inputs = tok_rep.task.generate_sample_inputs(2)
        inputs, _ = tok_rep.task._prepare_inputs_and_labels(sample_inputs, tok_rep.device)
        unused_parameters = task_utils.get_unused_parameters(tok_rep.model, **inputs)
        parameter_dict = common.get_num_parameters(tok_rep.model, unused_parameters)

        generate_report(
            tok_rep.task_name,
            tok_rep.model_name,
            parameter_dict["total"] - parameter_dict["unused"],
            num_sequences,
            num_bytes,
            end - start,
            tok_rep._mixed_precision_dtype,
            args.batch_size,
            not args.unsorted,
            tok_rep.device,
            file_path=args.report
        )


if __name__ == "__main__":
    # disable logging since we do not want that for our command line interface
    logging.disable(logging.CRITICAL)
    run(parse_args())
